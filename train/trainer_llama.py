from llama_index.legacy.finetuning import SentenceTransformersFinetuneEngine
from llama_index.legacy.finetuning import EmbeddingQAFinetuneDataset
import os
def get_local_model_path(model_name: str) -> str:
    """
    è·å–æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›HuggingFaceæ¨¡å‹åç§°ã€‚
    ä¸EmbeddingAdapterä¿æŒä¸€è‡´çš„é€»è¾‘ã€‚
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œä¾‹å¦‚ "BAAI/bge-large-zh-v1.5"
    
    Returns:
        æœ¬åœ°æ¨¡å‹è·¯å¾„æˆ–HuggingFaceæ¨¡å‹åç§°
    """
    # å°†huggingfaceæ¨¡å‹åç§°è½¬æ¢ä¸ºæœ¬åœ°ç¼“å­˜ç›®å½•åç§°
    cache_name = model_name.replace("/", "--")
    cache_name = f"models--{cache_name}"
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•å¹¶æ„å»ºæ¨¡å‹è·¯å¾„
    script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # å›åˆ° RAG ç›®å½•
    model_base_path = os.path.join(script_dir, "models", "embeddings", cache_name)
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨mainå¼•ç”¨æ–‡ä»¶
    main_ref_path = os.path.join(model_base_path, "refs", "main")
    if os.path.exists(main_ref_path):
        # è¯»å–mainå¼•ç”¨æŒ‡å‘çš„snapshot
        try:
            with open(main_ref_path, 'r') as f:
                snapshot_hash = f.read().strip()
            
            # æ„å»ºå®Œæ•´çš„snapshotè·¯å¾„
            snapshot_path = os.path.join(model_base_path, "snapshots", snapshot_hash)
            
            # éªŒè¯snapshotè·¯å¾„å­˜åœ¨ä¸”åŒ…å«å¿…è¦çš„é…ç½®æ–‡ä»¶
            if os.path.exists(snapshot_path) and os.path.exists(os.path.join(snapshot_path, "config.json")):
                print(f"ğŸ¯ ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {snapshot_path}")
                return snapshot_path
        except Exception as e:
            print(f"âš ï¸  è¯»å–æœ¬åœ°æ¨¡å‹å¼•ç”¨å¤±è´¥: {e}")
    
    print(f"ğŸŒ ä½¿ç”¨åœ¨çº¿æ¨¡å‹: {model_name}")
    return model_name

import json
project_dir = os.path.dirname(os.path.abspath(__file__))
TRAIN_CORPUS_FPATH = os.path.join(project_dir, "data/ft_train_corpus.json")
VAL_CORPUS_FPATH = os.path.join(project_dir, "data/ft_val_corpus.json")

# åŠ è½½JSONæ•°æ®
train_data = json.load(open(TRAIN_CORPUS_FPATH, "r", encoding="utf-8"))
val_data = json.load(open(VAL_CORPUS_FPATH, "r", encoding="utf-8"))

# è½¬æ¢ä¸ºEmbeddingQAFinetuneDatasetå¯¹è±¡
print("ğŸ“š æ­£åœ¨åŠ è½½æ•°æ®é›†...")
train_dataset = EmbeddingQAFinetuneDataset.from_json(TRAIN_CORPUS_FPATH)
val_dataset = EmbeddingQAFinetuneDataset.from_json(VAL_CORPUS_FPATH)

print(f"âœ… è®­ç»ƒæ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå¤§å°: {len(train_dataset.queries)} queries")
print(f"âœ… éªŒè¯æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå¤§å°: {len(val_dataset.queries)} queries")

model_name = "BAAI/bge-large-zh-v1.5"  # ä½¿ç”¨æ ‡å‡†çš„HuggingFaceæ¨¡å‹åç§°
model_path = get_local_model_path(model_name)

eval_name = model_name.replace("/", "_")
output_dir_name = f"ft_{eval_name}"  # ä½¿ç”¨ç®€åŒ–çš„åç§°ä½œä¸ºè¾“å‡ºç›®å½•

print("ğŸ”§ åˆ›å»ºå¾®è°ƒå¼•æ“...")
finetune_engine = SentenceTransformersFinetuneEngine(
    train_dataset,
    model_id=get_local_model_path(model_name),
    model_output_path=output_dir_name,
    val_dataset=val_dataset,



    # æ·»åŠ è°ƒè¯•å‚æ•°

)

finetune_engine.finetune()