# -*- coding: utf-8 -*-
# @file: estimate_resources.py
"""
è¿™ä¸ªè„šæœ¬ç”¨äºŽä¼°ç®— sentence-transformers è®­ç»ƒä»»åŠ¡æ‰€éœ€çš„èµ„æºï¼Œ
ä¸»è¦å…³æ³¨ GPU æ˜¾å­˜ï¼ˆVRAMï¼‰å’Œ CPU å†…å­˜ï¼ˆRAMï¼‰ã€‚
å®ƒä¸ä¼šè¿›è¡Œå®Œæ•´çš„è®­ç»ƒï¼Œè€Œæ˜¯æ¨¡æ‹Ÿæœ€è€—è´¹èµ„æºçš„å‡ ä¸ªæ­¥éª¤å¹¶è¿›è¡Œæµ‹é‡ã€‚

å¦‚ä½•ä½¿ç”¨ï¼š
1.  æ ¹æ®ä½ çš„æƒ…å†µä¿®æ”¹ä¸‹é¢çš„ã€é…ç½®å‚æ•°ã€‘éƒ¨åˆ†ã€‚
2.  ä½¿ç”¨æŒ‡å®šäº†å•ä¸ª GPU çš„å‘½ä»¤æ¥è¿è¡Œæ­¤è„šæœ¬ï¼Œä¾‹å¦‚ï¼š
    CUDA_VISIBLE_DEVICES=1 python estimate_resources.py
"""
import os
import json
import time
import torch
import psutil
from datasets import Dataset
from sentence_transformers import SentenceTransformer
from sentence_transformers.losses import MultipleNegativesRankingLoss
from torch.optim import AdamW

# --- 1. é…ç½®å‚æ•° (è¯·æ ¹æ®æ‚¨çš„éœ€æ±‚ä¿®æ”¹) ---
MODEL_NAME = "BAAI/bge-large-zh-v1.5"
# è®¾ç½®æ‚¨æ‰“ç®—ç”¨äºŽå®žé™…è®­ç»ƒçš„æ‰¹æ¬¡å¤§å°
BATCH_SIZE = 8  

project_dir = os.path.dirname(os.path.abspath(__file__))
TRAIN_FILE_PATH = os.path.join(project_dir, "data/ft_train_corpus_clean.json")
VAL_FILE_PATH = os.path.join(project_dir, "data/ft_val_corpus_clean.json")

# --- è¾…åŠ©å‡½æ•° ---
def format_bytes(size):
    """å°†å­—èŠ‚è½¬æ¢ä¸ºæ›´æ˜“è¯»çš„æ ¼å¼ (MB or GB)"""
    if size is None:
        return "N/A"
    power = 1024
    n = 0
    power_labels = {0: 'B', 1: 'KB', 2: 'MB', 3: 'GB', 4: 'TB'}
    while size > power and n < len(power_labels) -1 :
        size /= power
        n += 1
    return f"{size:.2f} {power_labels[n]}"

def get_process_memory():
    """èŽ·å–å½“å‰ Python è¿›ç¨‹çš„ CPU å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss


# --- 2. å¼€å§‹è¯Šæ–­ ---

print("="*50)
print("ðŸš€ å¼€å§‹è¿›è¡Œè®­ç»ƒèµ„æºéœ€æ±‚ä¼°ç®—...")
print("="*50)

# æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
if not torch.cuda.is_available():
    print("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°å¯ç”¨çš„ CUDA è®¾å¤‡ã€‚è¯·åœ¨ GPU çŽ¯å¢ƒä¸‹è¿è¡Œæ­¤è„šæœ¬ã€‚")
    exit()

device = torch.device("cuda:0")
print(f"âœ… æ£€æµ‹åˆ° GPU: {torch.cuda.get_device_name(0)}")

# --- æ­¥éª¤ 1: æµ‹é‡æ¨¡åž‹åŠ è½½æ‰€éœ€æ˜¾å­˜ ---
print("\n--- [è¯Šæ–­æ­¥éª¤ 1/4] æµ‹é‡æ¨¡åž‹åŠ è½½èµ„æº ---")
torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats(device)
mem_before_load = torch.cuda.memory_allocated(device)

model = SentenceTransformer(MODEL_NAME, device=device)

mem_after_load = torch.cuda.memory_allocated(device)
model_vram = mem_after_load - mem_before_load

print(f"ðŸ§  æ¨¡åž‹åŸºå‡†æ˜¾å­˜å ç”¨: {format_bytes(model_vram)}")
print(f"   (è¿™æ˜¯æ¨¡åž‹æƒé‡æœ¬èº«åŠ è½½åˆ° GPU ä¸Šæ‰€éœ€çš„æœ€å°æ˜¾å­˜)")

# --- æ­¥éª¤ 2: æµ‹é‡æ•°æ®åŠ è½½æ‰€éœ€ CPU å†…å­˜ ---
print("\n--- [è¯Šæ–­æ­¥éª¤ 2/4] æµ‹é‡æ•°æ®åŠ è½½èµ„æº ---")
cpu_mem_before = get_process_memory()

with open(TRAIN_FILE_PATH, "r", encoding="utf-8") as f:
    train_content = json.loads(f.read())
with open(VAL_FILE_PATH, "r", encoding="utf-8") as f:
    eval_content = json.loads(f.read())

corpus, queries = eval_content['corpus'], eval_content['queries']
train_anchor, train_positive = [], []
for query_id, context_id in train_content['relevant_docs'].items():
    train_anchor.append(train_content['queries'][query_id])
    train_positive.append(train_content['corpus'][context_id[0]])
train_dataset = Dataset.from_dict({"anchor": train_anchor, "positive": train_positive})

cpu_mem_after = get_process_memory()
data_ram = cpu_mem_after - cpu_mem_before
print(f"ðŸ“Š æ•°æ®é›†åŠ è½½åˆ° CPU å†…å­˜å¢žé‡: {format_bytes(data_ram)}")
print(f"   (æ‚¨çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®æ€»å…±å ç”¨äº†å¤§çº¦è¿™ä¹ˆå¤šç³»ç»Ÿå†…å­˜)")


# --- æ­¥éª¤ 3: æµ‹é‡è¯„ä¼°å™¨é¢„ç¼–ç æ‰€éœ€å³°å€¼æ˜¾å­˜ ---
print("\n--- [è¯Šæ–­æ­¥éª¤ 3/4] æµ‹é‡è¯„ä¼°å™¨è¯­æ–™åº“ç¼–ç å³°å€¼èµ„æº ---")
print("   (è¿™æ˜¯ trainer.train() å¼€å§‹æ—¶å¡ä½çš„é‚£ä¸€æ­¥)")

torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats(device)
start_time = time.time()

# æ¨¡æ‹Ÿ Evaluator çš„æ“ä½œï¼šå¯¹æ‰€æœ‰ corpus æ–‡æ¡£è¿›è¡Œç¼–ç 
print(f"   æ­£åœ¨å¯¹éªŒè¯é›†ä¸­çš„ {len(corpus)} ä¸ªæ–‡æ¡£è¿›è¡Œç¼–ç ...")
_ = model.encode(list(corpus.values()), batch_size=BATCH_SIZE, show_progress_bar=True)

end_time = time.time()
eval_peak_vram = torch.cuda.max_memory_allocated(device)
eval_time = end_time - start_time

print(f"ðŸ“‰ è¯„ä¼°å™¨é¢„ç¼–ç å³°å€¼æ˜¾å­˜: {format_bytes(eval_peak_vram)}")
print(f"â±ï¸  è¯„ä¼°å™¨é¢„ç¼–ç è€—æ—¶: {eval_time:.2f} ç§’")
print(f"   (è¿™æ˜¯è¿è¡Œè¯„ä¼°å‰ä¸€æ¬¡æ€§å‘ç”Ÿçš„èµ„æºæ¶ˆè€—)")


# --- æ­¥éª¤ 4: æµ‹é‡å•ä¸ªè®­ç»ƒæ­¥éª¤æ‰€éœ€å³°å€¼æ˜¾å­˜ ---
print("\n--- [è¯Šæ–­æ­¥éª¤ 4/4] æµ‹é‡å•ä¸ªè®­ç»ƒæ­¥éª¤å³°å€¼èµ„æº ---")
print(f"   (ä½¿ç”¨æ‚¨è®¾ç½®çš„ BATCH_SIZE = {BATCH_SIZE})")

loss_func = MultipleNegativesRankingLoss(model)
optimizer = AdamW(model.parameters(), lr=2e-5)

# ä»Žæ•°æ®é›†ä¸­å–ä¸€ä¸ªæ‰¹æ¬¡
train_batch = train_dataset.select(range(BATCH_SIZE))
# SentenceTransformerTrainer ä¼šè‡ªåŠ¨å¤„ç† tokenizer å’Œæ ¼å¼åŒ–
# è¿™é‡Œæˆ‘ä»¬ç›´æŽ¥ä½¿ç”¨æ–‡æœ¬è¾“å…¥ï¼Œå› ä¸ºæ¨¡åž‹å†…éƒ¨ä¼šå¤„ç†
batch_features = [{"anchor": item["anchor"], "positive": item["positive"]} for item in train_batch]
# ä½¿ç”¨æ¨¡åž‹çš„ tokenize æ–¹æ³•æ¥æ¨¡æ‹Ÿ trainer çš„è¡Œä¸º
tokenized_batch = model.tokenize(batch_features)
# å°† tokenized batch ç§»åŠ¨åˆ° GPU
for key in tokenized_batch:
    tokenized_batch[key] = tokenized_batch[key].to(device)


torch.cuda.empty_cache()
torch.cuda.reset_peak_memory_stats(device)

# --- æ¨¡æ‹Ÿè®­ç»ƒå¾ªçŽ¯çš„æ ¸å¿ƒ ---
# 1. å‰å‘ä¼ æ’­ (Forward Pass)
features = model(tokenized_batch)
# 2. è®¡ç®—æŸå¤±
loss = loss_func(features, labels=None) # MNRLoss ä¸éœ€è¦æ˜¾å¼ labels
# 3. åå‘ä¼ æ’­ (Backward Pass) - è¿™æ˜¯æœ€è€—æ˜¾å­˜çš„ä¸€æ­¥ï¼
loss.backward()
# 4. æ¸…ç©ºæ¢¯åº¦ (ä¸ºä¸‹ä¸€æ­¥åšå‡†å¤‡)
optimizer.zero_grad()
# ---------------------------

train_step_peak_vram = torch.cuda.max_memory_allocated(device)
print(f"ðŸ“ˆ å•ä¸ªè®­ç»ƒæ­¥éª¤å³°å€¼æ˜¾å­˜: {format_bytes(train_step_peak_vram)}")
print(f"   (è¿™æ˜¯è®­ç»ƒå¾ªçŽ¯ä¸­æ¯ä¸ª step æ‰€éœ€çš„æœ€é«˜æ˜¾å­˜ï¼Œå†³å®šäº†æ˜¯å¦ä¼š OOM)")

# --- 3. ç»“è®ºä¸Žå»ºè®® ---
print("\n" + "="*50)
print("âœ… è¯Šæ–­å®Œæˆï¼ç»“è®ºå¦‚ä¸‹ï¼š")
print("="*50)
print(f"  - æ¨¡åž‹åŸºå‡†æ˜¾å­˜: {format_bytes(model_vram)}")
print(f"  - è®­ç»ƒå³°å€¼æ˜¾å­˜ (Batch Size={BATCH_SIZE}): {format_bytes(train_step_peak_vram)}")
print(f"  - è¯„ä¼°å³°å€¼æ˜¾å­˜: {format_bytes(eval_peak_vram)}")
print("-" * 50)
final_recommendation = max(train_step_peak_vram, eval_peak_vram)
print(f"ðŸ‘‰ ç»¼åˆå»ºè®®ï¼š")
print(f"   è¦ä»¥ BATCH_SIZE={BATCH_SIZE} é¡ºåˆ©è¿è¡Œå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°ï¼Œ")
print(f"   æ‚¨é€‰æ‹©çš„ GPU è‡³å°‘éœ€è¦æœ‰ ã€{format_bytes(final_recommendation)}ã€‘ çš„å¯ç”¨æ˜¾å­˜ã€‚")
print(f"\n   å¦‚æžœ 'è®­ç»ƒå³°å€¼æ˜¾å­˜' è¶…å‡ºé¢„ç®—ï¼Œè¯·å‡å° BATCH_SIZE æˆ–å¢žåŠ  gradient_accumulation_stepsã€‚")
print(f"   å¦‚æžœ 'è¯„ä¼°å³°å€¼æ˜¾å­˜' è¶…å‡ºé¢„ç®—ï¼Œè¯·å‡å°è¯„ä¼°æ—¶çš„ per_device_eval_batch_sizeã€‚")