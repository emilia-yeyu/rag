# æœ¬åœ°Qwenæ¨¡å‹ä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ ä¸‹è½½å¹¶é…ç½®æœ¬åœ°Qwen2.5-1.5B-Instructæ¨¡å‹ï¼Œå®ç°ç¦»çº¿RAGç³»ç»Ÿã€‚

## ğŸŒŸ ä¼˜åŠ¿

- **ç¦»çº¿è¿è¡Œ**: æ— éœ€APIå¯†é’¥ï¼Œå®Œå…¨æœ¬åœ°åŒ–
- **æ•°æ®éšç§**: æ•°æ®ä¸ä¼šå‘é€åˆ°å¤–éƒ¨æœåŠ¡å™¨
- **æˆæœ¬æ§åˆ¶**: ä¸€æ¬¡ä¸‹è½½ï¼Œæ°¸ä¹…ä½¿ç”¨
- **å¾®è°ƒå‹å¥½**: å¯ä»¥åŸºäºæœ¬åœ°æ¨¡å‹è¿›è¡Œå¾®è°ƒ
- **å“åº”ç¨³å®š**: ä¸å—ç½‘ç»œæ³¢åŠ¨å½±å“

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### æœ€ä½é…ç½®
- **å†…å­˜**: 8GB RAM
- **å­˜å‚¨**: 5GB å¯ç”¨ç©ºé—´
- **Python**: 3.8+
- **PyTorch**: 2.0+

### æ¨èé…ç½®
- **å†…å­˜**: 16GB+ RAM
- **GPU**: NVIDIA GPU with 4GB+ VRAM (å¯é€‰)
- **å­˜å‚¨**: 10GB+ å¯ç”¨ç©ºé—´

### GPUæ”¯æŒ (å¯é€‰)
å¦‚æœæœ‰NVIDIA GPUï¼Œæ¨¡å‹å°†è‡ªåŠ¨ä½¿ç”¨GPUåŠ é€Ÿï¼š
```bash
# å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install -r requirements.txt

# é¢å¤–çš„æ¨¡å‹ç›¸å…³ä¾èµ–
pip install torch>=2.0.0 transformers>=4.32.0 accelerate>=0.21.0
```

### 2. ä¸‹è½½æ¨¡å‹

```bash
# è¿è¡Œä¸‹è½½è„šæœ¬
python download_model.py
```

ä¸‹è½½è¿‡ç¨‹ï¼š
- ğŸ“¦ æ¨¡å‹å¤§å°: ~3GB
- â±ï¸ é¢„è®¡æ—¶é—´: 5-15åˆ†é’Ÿ
- ğŸ“ ä¿å­˜ä½ç½®: `./models/qwen2.5-1.5b-instruct/`

### 3. æµ‹è¯•æ¨¡å‹

```bash
# è¿è¡Œæµ‹è¯•è„šæœ¬
python test_local_model.py
```

### 4. å¯åŠ¨RAGç³»ç»Ÿ

```bash
# ä½¿ç”¨æœ¬åœ°æ¨¡å‹å¯åŠ¨RAG
python rag.py
```

## ğŸ”§ é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½®å‚æ•°

åœ¨ `llm/local_model.py` ä¸­å¯ä»¥è°ƒæ•´ä»¥ä¸‹å‚æ•°ï¼š

```python
class LocalQwenModel(BaseChatModel):
    temperature: float = 0.7      # æ§åˆ¶éšæœºæ€§ (0-1)
    max_new_tokens: int = 512     # æœ€å¤§ç”Ÿæˆtokenæ•°
    do_sample: bool = True        # æ˜¯å¦é‡‡æ ·
    top_p: float = 0.8           # æ ¸é‡‡æ ·å‚æ•°
    top_k: int = 50              # top-ké‡‡æ ·å‚æ•°
    repetition_penalty: float = 1.1  # é‡å¤æƒ©ç½š
```

### RAGç³»ç»Ÿé…ç½®

åœ¨ `rag.py` ä¸­ä¿®æ”¹LLMé…ç½®ï¼š

```python
# åŸºæœ¬é…ç½®
self.llm = LLMAdapter.get_llm(
    "local", 
    "./models/qwen2.5-1.5b-instruct", 
    temperature=0.1
)

# é«˜çº§é…ç½®
self.llm = LLMAdapter.get_llm(
    "local", 
    "./models/qwen2.5-1.5b-instruct", 
    temperature=0.7,
    max_new_tokens=256,
    top_p=0.9,
    repetition_penalty=1.05
)
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### GPUåŠ é€Ÿ

å¦‚æœæœ‰NVIDIA GPUï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨ä½¿ç”¨GPUï¼š

```python
# æ£€æŸ¥GPUçŠ¶æ€
import torch
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"å½“å‰GPU: {torch.cuda.get_device_name()}")
```

### å†…å­˜ä¼˜åŒ–

å¯¹äºå†…å­˜å—é™çš„ç¯å¢ƒï¼š

```python
# åœ¨local_model.pyä¸­ä¿®æ”¹
self._model = AutoModelForCausalLM.from_pretrained(
    self.model_path,
    torch_dtype=torch.float16,     # ä½¿ç”¨åŠç²¾åº¦
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True,        # ä½CPUå†…å­˜ä½¿ç”¨
    load_in_8bit=True             # 8bité‡åŒ– (éœ€è¦å®‰è£…bitsandbytes)
)
```

### æ¨ç†åŠ é€Ÿ

```python
# å‡å°‘ç”Ÿæˆé•¿åº¦
max_new_tokens=128

# ç¦ç”¨é‡‡æ ·(æ›´å¿«ä½†å¯èƒ½é‡å¤)
do_sample=False

# ä½¿ç”¨è´ªå¿ƒè§£ç 
temperature=0.0
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**
   ```
   è§£å†³æ–¹æ¡ˆï¼š
   - å…³é—­å…¶ä»–åº”ç”¨ç¨‹åº
   - ä½¿ç”¨8bité‡åŒ–
   - å‡å°max_new_tokens
   ```

2. **æ¨¡å‹ä¸‹è½½å¤±è´¥**
   ```bash
   # è®¾ç½®é•œåƒæº
   export HF_ENDPOINT=https://hf-mirror.com
   
   # é‡æ–°ä¸‹è½½
   python download_model.py
   ```

3. **CUDAç‰ˆæœ¬ä¸åŒ¹é…**
   ```bash
   # é‡æ–°å®‰è£…å¯¹åº”ç‰ˆæœ¬çš„PyTorch
   pip uninstall torch torchvision torchaudio
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
   ```

4. **ç”Ÿæˆè´¨é‡ä¸ä½³**
   ```python
   # è°ƒæ•´å‚æ•°
   temperature=0.7        # å¢åŠ éšæœºæ€§
   top_p=0.9             # å¢åŠ å¤šæ ·æ€§
   repetition_penalty=1.1 # å‡å°‘é‡å¤
   ```

### è°ƒè¯•æ—¥å¿—

å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼š

```python
import logging
logging.basicConfig(level=logging.INFO)
```

## ğŸ”„ æ¨¡å‹å¾®è°ƒ

### å‡†å¤‡å¾®è°ƒç¯å¢ƒ

```bash
# å®‰è£…å¾®è°ƒç›¸å…³ä¾èµ–
pip install peft datasets trl
```

### å¾®è°ƒè„šæœ¬ç¤ºä¾‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model

# åŠ è½½æ¨¡å‹
model_path = "./models/qwen2.5-1.5b-instruct"
model = AutoModelForCausalLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# é…ç½®LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
)

# åº”ç”¨LoRA
model = get_peft_model(model, lora_config)

# å¾®è°ƒè®­ç»ƒ...
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

### ç¡¬ä»¶é…ç½®å¯¹æ¯”

| é…ç½® | åŠ è½½æ—¶é—´ | æ¨ç†é€Ÿåº¦ | å†…å­˜ä½¿ç”¨ |
|------|---------|---------|---------|
| CPU only (16GB RAM) | ~30ç§’ | ~2-3ç§’/å›ç­” | 4-6GB |
| RTX 3060 (12GB) | ~15ç§’ | ~0.5-1ç§’/å›ç­” | 3-4GB |
| RTX 4080 (16GB) | ~10ç§’ | ~0.3-0.5ç§’/å›ç­” | 3-4GB |

### è´¨é‡è¯„ä¼°

- **äº‹å®å‡†ç¡®æ€§**: â­â­â­â­ (åŸºäºRAGæ£€ç´¢)
- **è¯­è¨€æµç•…æ€§**: â­â­â­â­
- **ä¸Šä¸‹æ–‡ç†è§£**: â­â­â­â­
- **ä¸­æ–‡æ”¯æŒ**: â­â­â­â­â­

## ğŸ” ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬æŸ¥è¯¢

```python
from rag import SimpleRAG

# åˆå§‹åŒ–
rag = SimpleRAG("2.txt")

# æŸ¥è¯¢
result = rag.query("è´¾å®ç‰çš„æ€§æ ¼ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ")
print(result['answer'].content)
```

### æ‰¹é‡å¤„ç†

```python
questions = [
    "çº¢æ¥¼æ¢¦çš„ä¸»è¦äººç‰©æœ‰å“ªäº›ï¼Ÿ",
    "å¤§è§‚å›­æ˜¯ä»€ä¹ˆåœ°æ–¹ï¼Ÿ",
    "æ—é»›ç‰å’Œè–›å®é’—çš„å…³ç³»å¦‚ä½•ï¼Ÿ"
]

for question in questions:
    result = rag.query(question)
    print(f"Q: {question}")
    print(f"A: {result['answer'].content}\n")
```

### é…ç½®å®šåˆ¶

```python
# åˆ›å»ºè‡ªå®šä¹‰é…ç½®çš„RAG
rag = SimpleRAG("2.txt")

# ä¿®æ”¹LLMå‚æ•°
rag.llm = LLMAdapter.get_llm(
    "local", 
    "./models/qwen2.5-1.5b-instruct",
    temperature=0.8,           # æ›´æœ‰åˆ›é€ æ€§
    max_new_tokens=200,        # æ›´é•¿å›ç­”
    repetition_penalty=1.15    # æ›´å°‘é‡å¤
)
```

## ğŸ“š æ‰©å±•ä½¿ç”¨

### 1. å¤šæ¨¡å‹æ”¯æŒ

å¯ä»¥åŒæ—¶æ”¯æŒå¤šä¸ªæœ¬åœ°æ¨¡å‹ï¼š

```python
# é…ç½®ä¸åŒæ¨¡å‹ç”¨äºä¸åŒä»»åŠ¡
creative_llm = LLMAdapter.get_llm("local", "./models/qwen2.5-1.5b-instruct", temperature=0.9)
factual_llm = LLMAdapter.get_llm("local", "./models/qwen2.5-1.5b-instruct", temperature=0.1)
```

### 2. å¯¹è¯å†å²

æ·»åŠ å¯¹è¯å†å²æ”¯æŒï¼š

```python
from langchain_core.messages import HumanMessage, AIMessage

# ç»´æŠ¤å¯¹è¯å†å²
conversation_history = []

def chat_with_history(question):
    conversation_history.append(HumanMessage(content=question))
    response = rag.llm._generate(conversation_history)
    answer = response.generations[0].message.content
    conversation_history.append(AIMessage(content=answer))
    return answer
```

### 3. æµå¼è¾“å‡º

å®ç°æµå¼å›ç­”ï¼š

```python
# åœ¨local_model.pyä¸­æ·»åŠ æµå¼æ”¯æŒ
def _stream_generate(self, messages, **kwargs):
    # å®ç°æµå¼ç”Ÿæˆé€»è¾‘
    pass
```

## ğŸ¯ æœ€ä½³å®è·µ

1. **é¦–æ¬¡ä½¿ç”¨**ï¼šè¿è¡Œæµ‹è¯•è„šæœ¬ç¡®ä¿ä¸€åˆ‡æ­£å¸¸
2. **æ€§èƒ½ç›‘æ§**ï¼šå®šæœŸæ£€æŸ¥å†…å­˜å’ŒGPUä½¿ç”¨æƒ…å†µ
3. **æ¨¡å‹æ›´æ–°**ï¼šå…³æ³¨Qwenæ¨¡å‹çš„æ–°ç‰ˆæœ¬
4. **æ•°æ®å®‰å…¨**ï¼šæœ¬åœ°æ¨¡å‹ç¡®ä¿æ•°æ®éšç§
5. **å¤‡ä»½æ¨¡å‹**ï¼šä¸‹è½½å®Œæˆåå¤‡ä»½æ¨¡å‹æ–‡ä»¶

## ğŸ“ æ›´æ–°æ—¥å¿—

### v1.0.0
- âœ¨ åˆå§‹ç‰ˆæœ¬
- âœ¨ æ”¯æŒQwen2.5-1.5B-Instruct
- âœ¨ GPUè‡ªåŠ¨æ£€æµ‹å’ŒåŠ é€Ÿ
- âœ¨ å®Œæ•´çš„RAGé›†æˆ

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPRæ¥æ”¹è¿›æœ¬åœ°æ¨¡å‹æ”¯æŒï¼

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ¨¡å‹è®¸å¯**: è¯·éµå®ˆQwenæ¨¡å‹çš„ä½¿ç”¨è®¸å¯
2. **å•†ä¸šä½¿ç”¨**: å•†ä¸šä½¿ç”¨å‰è¯·æŸ¥çœ‹æ¨¡å‹çš„å•†ä¸šè®¸å¯æ¡æ¬¾
3. **å†…å®¹è¿‡æ»¤**: æœ¬åœ°æ¨¡å‹å¯èƒ½éœ€è¦é¢å¤–çš„å†…å®¹å®‰å…¨è¿‡æ»¤
4. **ç‰ˆæœ¬å…¼å®¹**: ä¸åŒç‰ˆæœ¬çš„transformerså¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜