# RAG/vector_store/hybrid_retriever.py
import math
from typing import List, Dict, Tuple, Optional, Set
from langchain_core.documents import Document
from collections import defaultdict

class RRFFusion:
    """
    å€’æ•°æ’åèåˆï¼ˆReciprocal Rank Fusionï¼‰ç®—æ³•å®ç°
    ç”¨äºèåˆå¤šä¸ªæ£€ç´¢ç»“æœ
    """
    
    def __init__(self, k: int = 60):
        """
        åˆå§‹åŒ–RRFèåˆå™¨
        
        Args:
            k: RRFå‚æ•°ï¼Œé€šå¸¸è®¾ä¸º60ï¼Œæ§åˆ¶æ’åçš„å¹³æ»‘ç¨‹åº¦
        """
        self.k = k
    
    def fuse_rankings(
        self, 
        rankings: List[List[Tuple[Document, float]]], 
        weights: Optional[List[float]] = None
    ) -> List[Tuple[Document, float]]:
        """
        èåˆå¤šä¸ªæ’ååˆ—è¡¨
        
        Args:
            rankings: å¤šä¸ªæ£€ç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«(Document, score)å¯¹
            weights: å„ä¸ªæ£€ç´¢å™¨çš„æƒé‡ï¼Œé»˜è®¤å‡ç­‰æƒé‡
        
        Returns:
            èåˆåçš„æ’ååˆ—è¡¨
        """
        if not rankings:
            return []
        
        # è®¾ç½®é»˜è®¤æƒé‡
        if weights is None:
            weights = [1.0] * len(rankings)
        
        # ç”¨äºå­˜å‚¨æ¯ä¸ªæ–‡æ¡£çš„èåˆåˆ†æ•°
        doc_scores = defaultdict(float)
        doc_objects = {}  # å­˜å‚¨æ–‡æ¡£å¯¹è±¡
        
        # å¯¹æ¯ä¸ªæ’ååˆ—è¡¨è¿›è¡ŒRRFè®¡ç®—
        for ranking_idx, ranking in enumerate(rankings):
            weight = weights[ranking_idx]
            
            for rank, (doc, original_score) in enumerate(ranking):
                # ä½¿ç”¨æ–‡æ¡£å†…å®¹ä½œä¸ºå”¯ä¸€æ ‡è¯†
                doc_id = doc.page_content
                
                # RRFå…¬å¼: score = weight * (1 / (k + rank + 1))
                rrf_score = weight / (self.k + rank + 1)
                doc_scores[doc_id] += rrf_score
                
                # ä¿å­˜æ–‡æ¡£å¯¹è±¡ï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
                if doc_id not in doc_objects:
                    doc_objects[doc_id] = doc
        
        # æŒ‰èåˆåˆ†æ•°æ’åº
        fused_results = []
        for doc_id, score in sorted(doc_scores.items(), key=lambda x: x[1], reverse=True):
            doc = doc_objects[doc_id]
            # å°†RRFåˆ†æ•°æ·»åŠ åˆ°å…ƒæ•°æ®
            doc.metadata['rrf_score'] = score
            fused_results.append((doc, score))
        
        return fused_results

class BM25Retriever:
    """
    ç®€å•çš„BM25æ£€ç´¢å™¨å®ç°
    ç”¨äºåŸºäºå…³é”®è¯çš„ç²¾ç¡®åŒ¹é…æ£€ç´¢
    """
    
    def __init__(self, documents: List[Document], k1: float = 1.5, b: float = 0.75, min_match_ratio: float = 0.3, score_threshold: float = 0.01):
        """
        åˆå§‹åŒ–BM25æ£€ç´¢å™¨
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            k1: BM25å‚æ•°ï¼Œæ§åˆ¶è¯é¢‘é¥±å’Œåº¦
            b: BM25å‚æ•°ï¼Œæ§åˆ¶æ–‡æ¡£é•¿åº¦çš„å½±å“
            min_match_ratio: æœ€å°åŒ¹é…è¯æ¯”ä¾‹ï¼ˆ0.3è¡¨ç¤ºè‡³å°‘åŒ¹é…30%çš„æŸ¥è¯¢è¯ï¼‰
            score_threshold: æœ€å°åˆ†æ•°é˜ˆå€¼ï¼Œä½äºæ­¤åˆ†æ•°çš„æ–‡æ¡£å°†è¢«è¿‡æ»¤
        """
        self.documents = documents
        self.k1 = k1
        self.b = b
        self.min_match_ratio = min_match_ratio
        self.score_threshold = score_threshold
        self.doc_count = len(documents)
        
        # é¢„å¤„ç†æ–‡æ¡£
        self._preprocess_documents()
    
    def _load_stop_words(self) -> set:
        """åŠ è½½åœç”¨è¯åº“"""
        import os
        
        # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))
        stop_words_path = os.path.join(current_dir, 'stop_words.txt')
        
        stop_words = set()
        
        try:
            if os.path.exists(stop_words_path):
                with open(stop_words_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        word = line.strip()
                        if word and not word.startswith('#'):  # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
                            stop_words.add(word)
   
            else:
                print(f"âš ï¸ åœç”¨è¯æ–‡ä»¶ä¸å­˜åœ¨: {stop_words_path}")
                # ä½¿ç”¨åŸºæœ¬çš„åœç”¨è¯ä½œä¸ºåå¤‡
                stop_words = {
                    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸€äº›',
                    'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä¼š', 'ç€', 'çœ‹', 'å¥½',
                    'è‡ªå·±', 'è¿™', 'é‚£', 'é‡Œ', 'è¿˜', 'æŠŠ', 'æ¥', 'æ—¶', 'ä¸ª', 'ä¸º', 'ä½†', 'æˆ–',
                    'ä¸', 'åŠ', 'ä»¥', 'æ‰€', 'è€Œ', 'ç­‰', 'ç­‰ç­‰', 'å¦‚', 'å¦‚æœ', 'å› ä¸º', 'æ‰€ä»¥'
                }
                print(f"âš ï¸ BM25æ£€ç´¢å™¨ä½¿ç”¨é»˜è®¤åœç”¨è¯åº“: {len(stop_words)} ä¸ªåœç”¨è¯")
        except Exception as e:
            print(f"âŒ BM25æ£€ç´¢å™¨åŠ è½½åœç”¨è¯åº“å¤±è´¥: {e}")
            # ä½¿ç”¨åŸºæœ¬çš„åœç”¨è¯ä½œä¸ºåå¤‡
            stop_words = {
                'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸€äº›',
                'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä¼š', 'ç€', 'çœ‹', 'å¥½'
            }
            print(f"âš ï¸ BM25æ£€ç´¢å™¨ä½¿ç”¨åŸºæœ¬åœç”¨è¯åº“: {len(stop_words)} ä¸ªåœç”¨è¯")
        
        return stop_words
    
    def _smart_tokenize(self, text: str) -> List[str]:
        """é€šç”¨æ™ºèƒ½åˆ†è¯ - é€‚ç”¨äºå„ç§ä¸­æ–‡æ–‡æœ¬ï¼Œä¿æŠ¤å¼•å·å†…å®¹"""
        import jieba
        import re
        
        # åˆå§‹åŒ–åœç”¨è¯ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ï¼‰
        if not hasattr(self, '_stop_words'):
            self.stop_words = self._load_stop_words()
        
        # ç¬¬ä¸€æ­¥ï¼šæå–å¹¶ä¿æŠ¤å¼•å·å†…çš„å†…å®¹
        quoted_terms = []
        remaining_text = text
        
        # åŒ¹é…å„ç§å¼•å·ï¼šä¸­æ–‡åŒå¼•å·ã€è‹±æ–‡åŒå¼•å·ã€ä¸­æ–‡å•å¼•å·
        quote_patterns = [
            r'â€œ([^â€]*)â€',
            r'"([^"]*)"',      # è‹±æ–‡åŒå¼•å·
            r'â€˜([^â€™]*)â€™',
            r"'([^']*)'",      # è‹±æ–‡å•å¼•å·
            r'ã€Š([^ã€‹]*)ã€‹',    # ä¹¦åå· ã€Šã€‹
        ]
        
        # é€ä¸ªå¤„ç†æ¯ç§å¼•å·æ¨¡å¼
        for pattern in quote_patterns:
            matches = list(re.finditer(pattern, remaining_text))
            for match in matches:
                quoted_content = match.group(1).strip()
                if quoted_content:  # éç©ºå¼•å·å†…å®¹
                    quoted_terms.append(quoted_content)
            # ç§»é™¤å·²å¤„ç†çš„å¼•å·å†…å®¹ï¼ˆåŒ…æ‹¬å¼•å·æœ¬èº«ï¼‰
            remaining_text = re.sub(pattern, ' ', remaining_text)
        
        # ç¬¬äºŒæ­¥ï¼šå¯¹å‰©ä½™æ–‡æœ¬è¿›è¡Œæ­£å¸¸åˆ†è¯ï¼ˆä½¿ç”¨ç²¾ç¡®æ¨¡å¼ï¼Œé¿å…å†—ä½™åˆ†è¯ï¼‰
        tokens = list(jieba.cut(remaining_text, cut_all=False))
        
        # ç¬¬ä¸‰æ­¥ï¼šæ¸…ç†å’Œè¿‡æ»¤æ™®é€šåˆ†è¯ç»“æœ
        processed_tokens = []
        for token in tokens:
            token = token.strip()
            
            # è·³è¿‡ç©ºå­—ç¬¦ä¸²å’Œåœç”¨è¯
            if not token or token in self.stop_words:
                continue
            
            # è·³è¿‡çº¯æ•°å­—ï¼ˆé™¤éæ˜¯ç‰¹æ®Šæƒ…å†µï¼Œå¦‚å¹´ä»½ï¼‰
            if token.isdigit() and len(token) < 4:
                continue
                
            # è·³è¿‡çº¯è‹±æ–‡å­—æ¯çš„çŸ­è¯ï¼ˆé™¤éæ˜¯ä¸“æœ‰åè¯ï¼‰
            if token.isalpha() and token.islower() and len(token) <= 2:
                continue
            
            # ä¿ç•™æœ‰æ„ä¹‰çš„è¯
            if len(token) >= 1:  # ä¿æŒå®½æ¾ï¼Œä¸è¿‡åº¦è¿‡æ»¤
                processed_tokens.append(token)
        
        # ç¬¬å››æ­¥ï¼šåˆå¹¶å¼•å·å†…å®¹å’Œæ™®é€šåˆ†è¯ç»“æœ
        final_tokens = quoted_terms + processed_tokens
        
        return final_tokens

    def _preprocess_documents(self):
        """é¢„å¤„ç†æ–‡æ¡£ï¼Œæ„å»ºå€’æ’ç´¢å¼•"""
        self.doc_tokens = []  # æ¯ä¸ªæ–‡æ¡£çš„åˆ†è¯ç»“æœ
        self.doc_lengths = []  # æ¯ä¸ªæ–‡æ¡£çš„é•¿åº¦
        self.term_doc_freq = defaultdict(int)  # è¯é¡¹çš„æ–‡æ¡£é¢‘ç‡
        self.vocab = set()  # è¯æ±‡è¡¨
        
        # åˆ†è¯å’Œæ„å»ºç´¢å¼•
        for doc in self.documents:
            # ä½¿ç”¨æ™ºèƒ½åˆ†è¯
            tokens = self._smart_tokenize(doc.page_content)
            
            self.doc_tokens.append(tokens)
            self.doc_lengths.append(len(tokens))
            
            # ç»Ÿè®¡è¯é¡¹æ–‡æ¡£é¢‘ç‡
            unique_tokens = set(tokens)
            for token in unique_tokens:
                self.term_doc_freq[token] += 1
                self.vocab.add(token)
        
        # è®¡ç®—å¹³å‡æ–‡æ¡£é•¿åº¦
        self.avg_doc_length = sum(self.doc_lengths) / len(self.doc_lengths) if self.doc_lengths else 0
        
        print(f"ğŸ“š BM25ç´¢å¼•æ„å»ºå®Œæˆ: {len(self.documents)}ç¯‡æ–‡æ¡£, {len(self.vocab)}ä¸ªè¯æ±‡")
        print(f"ğŸ”¤ è¯æ±‡ç¤ºä¾‹: {list(self.vocab)[:10]}")
    
    def search(self, query: str, k: int = 10) -> List[Tuple[Document, float]]:
        """
        BM25æ£€ç´¢ - æ”¯æŒéƒ¨åˆ†åŒ¹é…
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ–‡æ¡£æ•°é‡
        
        Returns:
            æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        import jieba
        # æŸ¥è¯¢åˆ†è¯ - ä½¿ç”¨ç›¸åŒçš„æ™ºèƒ½åˆ†è¯
        
        # æŸ¥è¯¢åˆ†è¯
        #query_tokens = list(jieba.cut(query))
        #query_tokens = [token.strip() for token in query_tokens if token.strip()]

        query_tokens = self._smart_tokenize(query)
        
        if not query_tokens:
            return []
        
        print(f"ğŸ” BM25æŸ¥è¯¢åˆ†è¯ (ä¼˜åŒ–å): {query_tokens}")
        
        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„BM25åˆ†æ•°
        scores = []
        
        for doc_idx, doc in enumerate(self.documents):
            doc_tokens = self.doc_tokens[doc_idx]
            doc_length = self.doc_lengths[doc_idx]
            
            score = 0.0
            matched_terms = 0
            
            for query_token in query_tokens:
                if query_token not in self.vocab:
                    continue
                
                # è®¡ç®—è¯é¢‘
                tf = doc_tokens.count(query_token)
                
                if tf > 0:
                    matched_terms += 1
                    
                    # è®¡ç®—IDF
                    df = self.term_doc_freq[query_token]
                    idf = math.log((self.doc_count - df + 0.5) / (df + 0.5))
                    
                    # è®¡ç®—BM25åˆ†æ•°
                    tf_component = (tf * (self.k1 + 1)) / (tf + self.k1 * (1 - self.b + self.b * (doc_length / self.avg_doc_length)))
                    score += idf * tf_component
            
            # è®¡ç®—åŒ¹é…æ¯”ä¾‹
            match_ratio = matched_terms / len(query_tokens) if query_tokens else 0
            
            # åªæœ‰æ»¡è¶³æœ€å°åŒ¹é…æ¯”ä¾‹ä¸”åˆ†æ•°è¶…è¿‡é˜ˆå€¼çš„æ–‡æ¡£æ‰è¢«ä¿ç•™
            if match_ratio >= self.min_match_ratio and score >= self.score_threshold:
                # å¯¹åˆ†æ•°è¿›è¡ŒåŒ¹é…æ¯”ä¾‹åŠ æƒï¼Œé¼“åŠ±åŒ¹é…æ›´å¤šè¯çš„æ–‡æ¡£
                adjusted_score = score * (0.5 + 0.5 * match_ratio)
                scores.append((doc, adjusted_score))
        
        print(f"ğŸ“Š BM25æ£€ç´¢ï¼š{len(scores)}ä¸ªæ–‡æ¡£æ»¡è¶³åŒ¹é…æ¡ä»¶")
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›å‰kä¸ª
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:k]

class HybridRetriever:
    """
    æ··åˆæ£€ç´¢å™¨ï¼šç»“åˆå‘é‡æ£€ç´¢å’ŒBM25æ£€ç´¢ï¼Œä½¿ç”¨RRFèåˆ
    """
    
    def __init__(self, vector_store_manager, rrf_k: int = 60, bm25_min_match_ratio: float = 0.3, bm25_score_threshold: float = 0.01):
        """
        åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨
        
        Args:
            vector_store_manager: å‘é‡å­˜å‚¨ç®¡ç†å™¨
            rrf_k: RRFèåˆå‚æ•°
            bm25_min_match_ratio: BM25æœ€å°åŒ¹é…è¯æ¯”ä¾‹
            bm25_score_threshold: BM25æœ€å°åˆ†æ•°é˜ˆå€¼
        """
        self.vector_store = vector_store_manager
        self.rrf_fusion = RRFFusion(k=rrf_k)
        self.bm25_retriever = None
        self.bm25_min_match_ratio = bm25_min_match_ratio
        self.bm25_score_threshold = bm25_score_threshold
        
        # åˆå§‹åŒ–BM25æ£€ç´¢å™¨
        self._init_bm25()
    
    def _init_bm25(self):
        """åˆå§‹åŒ–BM25æ£€ç´¢å™¨"""
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£
            if hasattr(self.vector_store, '_vector_store') and self.vector_store._vector_store:
                # ä»chromadbè·å–æ‰€æœ‰æ–‡æ¡£
                all_docs = self.vector_store._vector_store.get()
                
                if all_docs and 'documents' in all_docs:
                    documents = []
                    metadatas = all_docs.get('metadatas', [])
                    
                    for i, content in enumerate(all_docs['documents']):
                        metadata = metadatas[i] if i < len(metadatas) else {}
                        doc = Document(page_content=content, metadata=metadata)
                        documents.append(doc)
                    
                    self.bm25_retriever = BM25Retriever(
                        documents, 
                        min_match_ratio=self.bm25_min_match_ratio,
                        score_threshold=self.bm25_score_threshold
                    )
                    print(f"âœ… BM25æ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸï¼Œå…±{len(documents)}ä¸ªæ–‡æ¡£")
                    print(f"ğŸ“Š BM25é…ç½®ï¼šæœ€å°åŒ¹é…æ¯”ä¾‹={self.bm25_min_match_ratio:.1%}ï¼Œåˆ†æ•°é˜ˆå€¼={self.bm25_score_threshold}")
                else:
                    print("âš ï¸ å‘é‡å­˜å‚¨ä¸ºç©ºï¼Œæ— æ³•åˆå§‹åŒ–BM25æ£€ç´¢å™¨")
            else:
                print("âš ï¸ å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ›å»ºBM25æ£€ç´¢å™¨")
                
        except Exception as e:
            print(f"âŒ BM25æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.bm25_retriever = None
    
    def hybrid_search(
        self, 
        query: str, 
        k: int = 10,
        vector_weight: float = 0.6,
        bm25_weight: float = 0.4,
        vector_k: int = 15,
        bm25_k: int = 15
    ) -> List[Tuple[Document, float]]:
        """
        æ··åˆæ£€ç´¢ï¼šå‘é‡æ£€ç´¢ + BM25æ£€ç´¢ + RRFèåˆ
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: æœ€ç»ˆè¿”å›çš„æ–‡æ¡£æ•°é‡
            vector_weight: å‘é‡æ£€ç´¢æƒé‡
            bm25_weight: BM25æ£€ç´¢æƒé‡
            vector_k: å‘é‡æ£€ç´¢è¿”å›æ•°é‡
            bm25_k: BM25æ£€ç´¢è¿”å›æ•°é‡
        
        Returns:
            èåˆåçš„æ£€ç´¢ç»“æœ
        """
        rankings = []
        weights = []
        
        # 1. å‘é‡æ£€ç´¢
        try:
            print(f"ğŸ” æ‰§è¡Œå‘é‡æ£€ç´¢ (k={vector_k})...")
            vector_results = self.vector_store.search_similarity(query, k=vector_k)
            if vector_results:
                # è½¬æ¢ä¸º(Document, score)æ ¼å¼ï¼Œè¿™é‡Œç”¨ç›¸ä¼¼åº¦ä½œä¸ºåˆ†æ•°
                vector_ranking = [(doc, 1.0) for doc in vector_results]  # chromadbä¸ç›´æ¥è¿”å›åˆ†æ•°
                rankings.append(vector_ranking)
                weights.append(vector_weight)
                print(f"ğŸ“Š å‘é‡æ£€ç´¢æ‰¾åˆ° {len(vector_results)} ä¸ªæ–‡æ¡£")
            else:
                print("âš ï¸ å‘é‡æ£€ç´¢æ— ç»“æœ")
        except Exception as e:
            print(f"âŒ å‘é‡æ£€ç´¢å¤±è´¥: {e}")
        
        # 2. BM25æ£€ç´¢
        if self.bm25_retriever:
            try:
                print(f"ğŸ” æ‰§è¡ŒBM25æ£€ç´¢ (k={bm25_k})...")
                bm25_results = self.bm25_retriever.search(query, k=bm25_k)
                if bm25_results:
                    # è¿‡æ»¤æ‰åˆ†æ•°ä¸º0çš„ç»“æœ
                    bm25_ranking = [(doc, score) for doc, score in bm25_results if score > 0]
                    if bm25_ranking:
                        rankings.append(bm25_ranking)
                        weights.append(bm25_weight)
                        print(f"ğŸ“Š BM25æ£€ç´¢æ‰¾åˆ° {len(bm25_ranking)} ä¸ªç›¸å…³æ–‡æ¡£")
                    else:
                        print("âš ï¸ BM25æ£€ç´¢æ— ç›¸å…³ç»“æœ")
                else:
                    print("âš ï¸ BM25æ£€ç´¢æ— ç»“æœ")
            except Exception as e:
                print(f"âŒ BM25æ£€ç´¢å¤±è´¥: {e}")
        else:
            print("âš ï¸ BM25æ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡BM25æ£€ç´¢")
        
        # 3. RRFèåˆ
        if not rankings:
            print("âŒ æ‰€æœ‰æ£€ç´¢æ–¹æ³•éƒ½å¤±è´¥äº†")
            return []
        
        if len(rankings) == 1:
            print("âš ï¸ åªæœ‰ä¸€ç§æ£€ç´¢æ–¹æ³•æœ‰ç»“æœï¼Œæ— éœ€èåˆ")
            final_results = rankings[0][:k]
        else:
            print(f"ğŸ”„ ä½¿ç”¨RRFèåˆ {len(rankings)} ä¸ªæ£€ç´¢ç»“æœ...")
            fused_results = self.rrf_fusion.fuse_rankings(rankings, weights)
            final_results = fused_results[:k]
            print(f"âœ… RRFèåˆå®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªæ–‡æ¡£")
        
        return final_results 