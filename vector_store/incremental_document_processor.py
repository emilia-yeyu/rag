import os
import hashlib
from typing import List, Dict, Any, Optional, Set, Tuple
from datetime import datetime
from pathlib import Path

from langchain_core.documents import Document
from langchain_community.document_loaders import DirectoryLoader
from langchain.indexes import SQLRecordManager, index
from langchain_core.vectorstores import VectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter

from .vector_store import VectorStoreManager


class IncrementalDocumentProcessor:
    """
    æ”¯æŒå¢é‡æ›´æ–°çš„æ–‡æ¡£å¤„ç†å™¨ã€‚
    åŸºäºLangChain RecordManagerè·Ÿè¸ªæ–‡æ¡£å˜æ›´ï¼Œå®ç°æ™ºèƒ½å¢é‡ç´¢å¼•ã€‚
    """
    
    def __init__(
        self,
        docs_path: str,
        vector_store_manager: VectorStoreManager,
        record_manager: Optional[SQLRecordManager] = None,
        text_splitter: Optional[RecursiveCharacterTextSplitter] = None,
        loader_kwargs: Optional[Dict[str, Any]] = None,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        supported_extensions: List[str] = None,
    ):
        """
        åˆå§‹åŒ–å¢é‡æ–‡æ¡£å¤„ç†å™¨ã€‚
        
        å‚æ•°:
            docs_path: æ–‡æ¡£ç›®å½•è·¯å¾„
            vector_store_manager: VectorStoreManagerå®ä¾‹
            record_manager: LangChainè®°å½•ç®¡ç†å™¨ï¼Œç”¨äºè·Ÿè¸ªæ–‡æ¡£çŠ¶æ€
            text_splitter: æ–‡æœ¬åˆ†å‰²å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            loader_kwargs: æ–‡æ¡£åŠ è½½å™¨å‚æ•°
            chunk_size: æ–‡æ¡£å—å¤§å°
            chunk_overlap: æ–‡æ¡£å—é‡å å¤§å°
            supported_extensions: æ”¯æŒçš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨
        """
        # éªŒè¯æ–‡æ¡£è·¯å¾„
        if not os.path.isdir(docs_path):
            raise ValueError(f"æ–‡æ¡£è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªç›®å½•: {docs_path}")
        
        self.docs_path = docs_path
        self.vector_store_manager = vector_store_manager
        self.loader_kwargs = loader_kwargs or {}
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
        self.supported_extensions = supported_extensions or ['.txt', '.md', '.pdf', '.doc', '.docx', '.html']
        
        # è®¾ç½®è®°å½•ç®¡ç†å™¨
        if record_manager is None:
            # å¦‚æœæ²¡æœ‰æä¾›ï¼Œåˆ›å»ºé»˜è®¤çš„è®°å½•ç®¡ç†å™¨
            cache_dir = os.path.join(os.path.dirname(docs_path), '.rag_cache')
            os.makedirs(cache_dir, exist_ok=True)
            
            self.record_manager = SQLRecordManager(
                namespace=f"rag_docs_{os.path.basename(docs_path)}",
                db_url=f"sqlite:///{cache_dir}/record_manager.db"
            )
            self.record_manager.create_schema()
        else:
            self.record_manager = record_manager
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        if text_splitter is None:
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=["\n\n", "\n", " ", ""],
                keep_separator=False,
            )
        else:
            self.text_splitter = text_splitter
        
        # æ–‡æ¡£çŠ¶æ€ç¼“å­˜
        self._doc_metadata_cache: Dict[str, Dict] = {}
        print(f"âœ… å¢é‡æ–‡æ¡£å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œç›‘æ§ç›®å½•: {docs_path}")
    
    def load_documents(self) -> List[Document]:
        """ä»æŒ‡å®šç›®å½•åŠ è½½åŸå§‹æ–‡æ¡£"""
        try:
            loader = DirectoryLoader(
                self.docs_path,
                loader_kwargs=self.loader_kwargs,
                show_progress=True, 
                use_multithreading=True,
                silent_errors=True  # è·³è¿‡æ— æ³•åŠ è½½çš„æ–‡ä»¶
            )
            documents = loader.load()
            print(f"ğŸ“š ä» {self.docs_path} åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
            return documents
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            return []
        
    def _compute_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            print(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return ""
    
    def _get_file_metadata(self, file_path: str) -> Dict[str, Any]:
        """è·å–æ–‡ä»¶å…ƒæ•°æ®"""
        stat = os.stat(file_path)
        return {
            "file_path": file_path,
            "file_size": stat.st_size,
            "modified_time": datetime.fromtimestamp(stat.st_mtime),
            "hash": self._compute_file_hash(file_path),
        }
    
    def _scan_directory_changes(self) -> Tuple[Set[str], Set[str], Set[str]]:
        """
        æ‰«æç›®å½•å˜æ›´ã€‚
        
        è¿”å›:
            Tuple[æ–°å¢æ–‡ä»¶, ä¿®æ”¹æ–‡ä»¶, åˆ é™¤æ–‡ä»¶]
        """
        current_files = set()
        new_files = set()
        modified_files = set()
        
        # æ‰«æå½“å‰ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        for root, dirs, files in os.walk(self.docs_path):
            for file in files:
                file_path = os.path.join(root, file)
                # è¿‡æ»¤æ”¯æŒçš„æ–‡ä»¶ç±»å‹
                if any(file.lower().endswith(ext) for ext in self.supported_extensions):
                    current_files.add(file_path)
                    
                    # è·å–å½“å‰æ–‡ä»¶å…ƒæ•°æ®
                    current_metadata = self._get_file_metadata(file_path)
                    
                    if file_path not in self._doc_metadata_cache:
                        # æ–°æ–‡ä»¶
                        new_files.add(file_path)
                        self._doc_metadata_cache[file_path] = current_metadata
                    else:
                        # æ£€æŸ¥æ˜¯å¦è¢«ä¿®æ”¹
                        cached_metadata = self._doc_metadata_cache[file_path]
                        if (current_metadata["hash"] != cached_metadata["hash"] or
                            current_metadata["modified_time"] != cached_metadata["modified_time"]):
                            modified_files.add(file_path)
                            self._doc_metadata_cache[file_path] = current_metadata
        
        # æ£€æŸ¥åˆ é™¤çš„æ–‡ä»¶
        cached_files = set(self._doc_metadata_cache.keys())
        deleted_files = cached_files - current_files
        
        # æ¸…ç†å·²åˆ é™¤æ–‡ä»¶çš„ç¼“å­˜
        for deleted_file in deleted_files:
            del self._doc_metadata_cache[deleted_file]
        
        return new_files, modified_files, deleted_files
    
    def _load_specific_files(self, file_paths: Set[str]) -> List[Document]:
        """åŠ è½½æŒ‡å®šçš„æ–‡ä»¶åˆ—è¡¨"""
        documents = []
        
        for file_path in file_paths:
            try:
                # ä½¿ç”¨é€‚å½“çš„åŠ è½½å™¨åŠ è½½å•ä¸ªæ–‡ä»¶
                if file_path.endswith('.txt'):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    doc = Document(
                        page_content=content,
                        metadata={
                            "source": file_path,
                            "file_hash": self._compute_file_hash(file_path),
                            "load_time": datetime.now().isoformat(),
                        }
                    )
                    documents.append(doc)
                # å¯ä»¥æ‰©å±•æ”¯æŒæ›´å¤šæ–‡ä»¶ç±»å‹
                else:
                    print(f"æš‚ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
                    
            except Exception as e:
                print(f"åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                
        return documents
    
    def _create_document_ids(self, documents: List[Document]) -> List[str]:
        """ä¸ºæ–‡æ¡£åˆ›å»ºå”¯ä¸€ID"""
        doc_ids = []
        for i, doc in enumerate(documents):
            # åŸºäºæ–‡ä»¶è·¯å¾„å’Œå†…å®¹å“ˆå¸Œåˆ›å»ºå”¯ä¸€ID
            source = doc.metadata.get("source", "unknown")
            content_hash = hashlib.md5(doc.page_content.encode()).hexdigest()[:8]
            doc_id = f"{Path(source).stem}_{content_hash}_{i}"
            doc_ids.append(doc_id)
        return doc_ids
    
    async def incremental_index(self, cleanup: str = "incremental") -> Dict[str, Any]:
        """
        æ‰§è¡Œå¢é‡ç´¢å¼•æ›´æ–°ã€‚
        
        å‚æ•°:
            cleanup: æ¸…ç†æ¨¡å¼
                - "incremental": åªå¤„ç†å˜æ›´çš„æ–‡æ¡£
                - "full": å®Œå…¨é‡å»ºç´¢å¼•
                - None: åªæ·»åŠ æ–°æ–‡æ¡£ï¼Œä¸åˆ é™¤æ—§æ–‡æ¡£
        
        è¿”å›:
            åŒ…å«æ›´æ–°ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        print("ğŸ” æ‰«ææ–‡æ¡£å˜æ›´...")
        new_files, modified_files, deleted_files = self._scan_directory_changes()
        
        stats = {
            "new_files": len(new_files),
            "modified_files": len(modified_files), 
            "deleted_files": len(deleted_files),
            "total_processed": 0,
        }
        
        # å¤„ç†æ–°å¢å’Œä¿®æ”¹çš„æ–‡ä»¶
        changed_files = new_files | modified_files
        if changed_files:
            print(f"ğŸ“š å¤„ç† {len(changed_files)} ä¸ªå˜æ›´æ–‡ä»¶...")
            
            # åŠ è½½å˜æ›´çš„æ–‡æ¡£
            documents = self._load_specific_files(changed_files)
            
            if documents:
                # åˆ†å‰²æ–‡æ¡£
                split_docs = self.text_splitter.split_documents(documents)
                
                # åˆ›å»ºæ–‡æ¡£ID
                doc_ids = self._create_document_ids(split_docs)
                
                # ä½¿ç”¨LangChainçš„indexå‡½æ•°è¿›è¡Œå¢é‡æ›´æ–°
                result = await index(
                    doc_ids,
                    split_docs,
                    record_manager=self.record_manager,
                    vector_store=self.vector_store_manager._vector_store,
                    cleanup=cleanup,
                    source_id_key="source",
                )
                
                stats["total_processed"] = len(split_docs)
                stats["index_result"] = result
                
                print(f"âœ… æˆåŠŸå¤„ç† {len(split_docs)} ä¸ªæ–‡æ¡£å—")
        
        # å¤„ç†åˆ é™¤çš„æ–‡ä»¶
        if deleted_files and cleanup == "incremental":
            print(f"ğŸ—‘ï¸ æ¸…ç† {len(deleted_files)} ä¸ªå·²åˆ é™¤æ–‡ä»¶çš„ç´¢å¼•...")
            # TODO: å®ç°åˆ é™¤é€»è¾‘
            # éœ€è¦æ ¹æ®sourceå­—æ®µä»è®°å½•ç®¡ç†å™¨ä¸­åˆ é™¤ç›¸å…³è®°å½•
        
        print("ğŸ‰ å¢é‡ç´¢å¼•æ›´æ–°å®Œæˆ!")
        return stats
    
    def full_reindex(self) -> Dict[str, Any]:
        """å®Œå…¨é‡å»ºç´¢å¼•"""
        print("ğŸ”„ æ‰§è¡Œå®Œå…¨é‡å»ºç´¢å¼•...")
        
        # åŠ è½½æ‰€æœ‰æ–‡æ¡£
        documents = self.load_documents()
        
        if not documents:
            return {"total_processed": 0, "message": "æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£"}
        
        # åˆ†å‰²æ–‡æ¡£
        split_docs = self.text_splitter.split_documents(documents)
        
        # åˆ›å»ºæ–‡æ¡£ID
        doc_ids = self._create_document_ids(split_docs)
        
        # å®Œå…¨é‡å»ºç´¢å¼•
        result = index(
            doc_ids,
            split_docs,
            record_manager=self.record_manager,
            vector_store=self.vector_store_manager._vector_store,
            cleanup="full",
            source_id_key="source",
        )
        
        # æ›´æ–°å…ƒæ•°æ®ç¼“å­˜
        self._update_metadata_cache()
        
        stats = {
            "total_processed": len(split_docs),
            "index_result": result,
        }
        
        print(f"âœ… å®Œå…¨é‡å»ºå®Œæˆï¼Œå¤„ç†äº† {len(split_docs)} ä¸ªæ–‡æ¡£å—")
        return stats
    
    def _update_metadata_cache(self):
        """æ›´æ–°å…ƒæ•°æ®ç¼“å­˜"""
        self._doc_metadata_cache.clear()
        
        for root, dirs, files in os.walk(self.docs_path):
            for file in files:
                file_path = os.path.join(root, file)
                if any(file.lower().endswith(ext) for ext in self.supported_extensions):
                    self._doc_metadata_cache[file_path] = self._get_file_metadata(file_path)
    
    def get_index_stats(self) -> Dict[str, Any]:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # ä»è®°å½•ç®¡ç†å™¨è·å–ç»Ÿè®¡ä¿¡æ¯
            total_records = len(self.record_manager.list_keys())
            
            return {
                "total_documents": len(self._doc_metadata_cache),
                "total_chunks": total_records,
                "cache_size": len(self._doc_metadata_cache),
            }
        except Exception as e:
            return {"error": str(e)}
    
    def update_documents(self) -> Dict[str, Any]:
        """
        åŒæ­¥ç‰ˆæœ¬çš„å¢é‡æ–‡æ¡£æ›´æ–°æ–¹æ³•
        """
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(self._check_and_update_documents())
        finally:
            loop.close()
    
    async def _check_and_update_documents(self) -> Dict[str, Any]:
        """
        æ£€æŸ¥å¹¶æ‰§è¡Œå¢é‡æ–‡æ¡£æ›´æ–°ï¼ˆæ ¸å¿ƒé€»è¾‘ï¼‰
        
        è¿”å›:
            æ›´æ–°ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            print("ğŸ” å¼€å§‹æ£€æŸ¥æ–‡æ¡£å˜æ›´...")
            stats = await self.incremental_index(cleanup="incremental")
            
            if stats["new_files"] > 0 or stats["modified_files"] > 0:
                print(f"âœ… æ£€æµ‹åˆ°å˜æ›´å¹¶å·²æ›´æ–°: æ–°å¢{stats['new_files']}ä¸ªï¼Œä¿®æ”¹{stats['modified_files']}ä¸ªæ–‡ä»¶")
                stats["status"] = "success"
                stats["message"] = f"æˆåŠŸæ›´æ–° {stats['total_processed']} ä¸ªæ–‡æ¡£å—"
            else:
                print("â„¹ï¸ æ²¡æœ‰æ£€æµ‹åˆ°æ–‡æ¡£å˜æ›´")
                stats["status"] = "no_changes"
                stats["message"] = "æ²¡æœ‰æ£€æµ‹åˆ°æ–‡æ¡£å˜æ›´"
            
            return stats
            
        except Exception as e:
            print(f"âŒ å¢é‡æ›´æ–°å¤±è´¥: {e}")
            return {
                "status": "error",
                "message": f"å¢é‡æ›´æ–°å¤±è´¥: {str(e)}"
            }
    
    def get_comprehensive_status(self) -> Dict[str, Any]:
        """
        è·å–å¢é‡å¤„ç†å™¨çš„ç»¼åˆçŠ¶æ€ä¿¡æ¯
        """
        try:
            # åŸºç¡€ç»Ÿè®¡
            index_stats = self.get_index_stats()
            
            # æ–‡ä»¶ç›‘æ§ç»Ÿè®¡
            file_stats = {
                "monitored_directory": self.docs_path,
                "supported_extensions": self.supported_extensions,
                "cached_files_count": len(self._doc_metadata_cache),
            }
            
            # é…ç½®ä¿¡æ¯
            config_info = {
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap,
                "record_manager_namespace": self.record_manager.namespace,
            }
            
            # åˆå¹¶æ‰€æœ‰ä¿¡æ¯
            comprehensive_status = {
                **index_stats,
                **file_stats,
                **config_info
            }
            
            return comprehensive_status
            
        except Exception as e:
            return {"comprehensive_status_error": str(e)}
    
    @classmethod
    def create_with_vector_store(
        cls,
        docs_path: str,
        vector_store_manager: VectorStoreManager,
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        **kwargs
    ) -> "IncrementalDocumentProcessor":
        """
        ä¾¿æ·æ–¹æ³•ï¼šä½¿ç”¨VectorStoreManageråˆ›å»ºå¢é‡å¤„ç†å™¨
        
        å‚æ•°:
            docs_path: æ–‡æ¡£ç›®å½•è·¯å¾„
            vector_store_manager: VectorStoreManagerå®ä¾‹
            chunk_size: æ–‡æ¡£å—å¤§å°
            chunk_overlap: æ–‡æ¡£å—é‡å å¤§å°
            **kwargs: å…¶ä»–å‚æ•°
        
        è¿”å›:
            IncrementalDocumentProcessorå®ä¾‹
        """
        return cls(
            docs_path=docs_path,
            vector_store_manager=vector_store_manager,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            **kwargs
        )
