#!/usr/bin/env python3
"""
å¤šæ–‡ä»¶æ–‡æ¡£å¤„ç†å™¨
ä¸“é—¨å¤„ç†dataç›®å½•ä¸‹çš„å¤šä¸ªtxtæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„chunk
"""

import os
import glob
from typing import List, Dict, Any, Optional
from langchain_core.documents import Document


class MultiFileProcessor:
    """
    å¤„ç†å¤šä¸ªtxtæ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„æ–‡æ¡£å—
    """
    
    def __init__(self, data_dir: str = "data"):
        """
        åˆå§‹åŒ–å¤šæ–‡ä»¶å¤„ç†å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
        """
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•åœ¨è„šæœ¬æ‰€åœ¨ç›®å½•æŸ¥æ‰¾
        if not os.path.isabs(data_dir):
            script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            full_path = os.path.join(script_dir, data_dir)
            if os.path.exists(full_path):
                data_dir = full_path
        
        self.data_dir = data_dir
        print(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_dir}")
        
        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.data_dir):
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
        
        if not os.path.isdir(self.data_dir):
            raise ValueError(f"è·¯å¾„ä¸æ˜¯ç›®å½•: {self.data_dir}")
    
    def load_documents(self, file_pattern: str = "*.txt") -> List[Document]:
        """
        åŠ è½½æ‰€æœ‰txtæ–‡ä»¶ä½œä¸ºç‹¬ç«‹çš„æ–‡æ¡£
        
        Args:
            file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼ï¼Œé»˜è®¤ä¸º"*.txt"
            
        Returns:
            List[Document]: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡ä»¶å¯¹åº”ä¸€ä¸ªDocument
        """
        # è·å–æ‰€æœ‰txtæ–‡ä»¶
        pattern = os.path.join(self.data_dir, file_pattern)
        txt_files = glob.glob(pattern)
        
        if not txt_files:
            print(f"âš ï¸ åœ¨ {self.data_dir} ä¸­æœªæ‰¾åˆ°åŒ¹é… {file_pattern} çš„æ–‡ä»¶")
            return []
        
        # æŒ‰æ–‡ä»¶åæ’åºï¼ˆæ”¯æŒæ•°å­—æ’åºï¼‰
        txt_files.sort(key=lambda x: self._natural_sort_key(os.path.basename(x)))
        
        print(f"ğŸ“„ æ‰¾åˆ° {len(txt_files)} ä¸ªtxtæ–‡ä»¶")
        
        documents = []
        for file_path in txt_files:
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                
                if not content:
                    print(f"âš ï¸ æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡: {os.path.basename(file_path)}")
                    continue
                
                # åˆ›å»ºDocumentå¯¹è±¡
                filename = os.path.basename(file_path)
                file_number = self._extract_file_number(filename)
                
                # æå–æ ‡é¢˜ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€è¡Œï¼‰
                lines = content.split('\n')
                title = lines[0].strip() if lines else filename
                
                document = Document(
                    page_content=content,
                    metadata={
                        'source': file_path,
                        'filename': filename,
                        'file_number': file_number,
                        'title': title,
                        'chunk_type': 'file',
                        'chunk_id': filename.replace('.txt', ''),
                        'char_count': len(content),
                        'line_count': len(lines)
                    }
                )
                
                documents.append(document)
                print(f"âœ… åŠ è½½æ–‡ä»¶: {filename} ({len(content)} å­—ç¬¦)")
                
            except Exception as e:
                print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {os.path.basename(file_path)}: {e}")
                continue
        
        print(f"ğŸ¯ æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    def _natural_sort_key(self, filename: str) -> tuple:
        """
        è‡ªç„¶æ’åºé”®ï¼Œæ”¯æŒæ•°å­—æ’åºï¼ˆ1.txt, 2.txt, ..., 10.txt, 11.txtï¼‰
        """
        import re
        # æå–æ–‡ä»¶åä¸­çš„æ•°å­—éƒ¨åˆ†
        parts = re.split(r'(\d+)', filename)
        return tuple(int(part) if part.isdigit() else part for part in parts)
    
    def _extract_file_number(self, filename: str) -> Optional[int]:
        """
        ä»æ–‡ä»¶åä¸­æå–æ•°å­—ç¼–å·
        """
        import re
        match = re.search(r'(\d+)', filename)
        return int(match.group(1)) if match else None
    
    def get_file_info(self) -> Dict[str, Any]:
        """
        è·å–æ•°æ®ç›®å½•ä¿¡æ¯
        """
        pattern = os.path.join(self.data_dir, "*.txt")
        txt_files = glob.glob(pattern)
        
        total_size = 0
        file_info = []
        
        for file_path in txt_files:
            try:
                size = os.path.getsize(file_path)
                total_size += size
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    lines = len(content.split('\n'))
                    chars = len(content)
                
                file_info.append({
                    'filename': os.path.basename(file_path),
                    'size_bytes': size,
                    'char_count': chars,
                    'line_count': lines
                })
            except Exception as e:
                print(f"âš ï¸ è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {os.path.basename(file_path)}: {e}")
        
        return {
            'data_directory': self.data_dir,
            'total_files': len(txt_files),
            'total_size_bytes': total_size,
            'total_size_kb': round(total_size / 1024, 2),
            'files': sorted(file_info, key=lambda x: self._natural_sort_key(x['filename']))
        }


def create_multi_file_processor(data_dir: str = "data") -> MultiFileProcessor:
    """
    åˆ›å»ºå¤šæ–‡ä»¶å¤„ç†å™¨å®ä¾‹
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        
    Returns:
        MultiFileProcessor: é…ç½®å¥½çš„å¤„ç†å™¨å®ä¾‹
    """
    return MultiFileProcessor(data_dir=data_dir)


if __name__ == "__main__":
    # æµ‹è¯•å¤šæ–‡ä»¶å¤„ç†å™¨
    print("ğŸ§ª æµ‹è¯•å¤šæ–‡ä»¶å¤„ç†å™¨")
    print("=" * 50)
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = create_multi_file_processor()
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        info = processor.get_file_info()
        print(f"\nğŸ“Š æ•°æ®ç›®å½•ä¿¡æ¯:")
        print(f"  ç›®å½•: {info['data_directory']}")
        print(f"  æ–‡ä»¶æ€»æ•°: {info['total_files']}")
        print(f"  æ€»å¤§å°: {info['total_size_kb']} KB")
        
        # æ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶ä¿¡æ¯
        print(f"\nğŸ“„ æ–‡ä»¶åˆ—è¡¨ (å‰5ä¸ª):")
        for file_info in info['files'][:5]:
            print(f"  {file_info['filename']}: {file_info['char_count']} å­—ç¬¦, {file_info['line_count']} è¡Œ")
        
        if len(info['files']) > 5:
            print(f"  ... è¿˜æœ‰ {len(info['files']) - 5} ä¸ªæ–‡ä»¶")
        
        # åŠ è½½æ–‡æ¡£
        print(f"\nğŸ“š åŠ è½½æ–‡æ¡£...")
        documents = processor.load_documents()
        
        if documents:
            print(f"\nâœ… åŠ è½½å®Œæˆï¼")
            print(f"  æ–‡æ¡£æ•°é‡: {len(documents)}")
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ–‡æ¡£ç¤ºä¾‹
            first_doc = documents[0]
            print(f"\nğŸ“– ç¬¬ä¸€ä¸ªæ–‡æ¡£ç¤ºä¾‹:")
            print(f"  æ–‡ä»¶å: {first_doc.metadata['filename']}")
            print(f"  æ ‡é¢˜: {first_doc.metadata['title']}")
            print(f"  å­—ç¬¦æ•°: {first_doc.metadata['char_count']}")
            print(f"  å†…å®¹é¢„è§ˆ: {first_doc.page_content[:100]}...")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
