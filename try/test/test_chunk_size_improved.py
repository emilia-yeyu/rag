#!/usr/bin/env python3
"""
æ”¹è¿›ç‰ˆ Chunk Size ä¼˜åŒ–æµ‹è¯•è„šæœ¬
ä½¿ç”¨äººå·¥è®¾è®¡çš„é—®é¢˜-ç­”æ¡ˆå¯¹æ•°æ®é›†è¿›è¡Œè¯„ä¼°
"""

import os
import time
import json
import numpy as np
import shutil
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
from sklearn.metrics.pairwise import cosine_similarity

# å¯¼å…¥ç°æœ‰RAGç»„ä»¶
from embedding.adapter import EmbeddingAdapter
from llm.adapter import LLMAdapter
from vector_store.vector_store import VectorStoreManager
from langchain_core.documents import Document
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

@dataclass
class ImprovedQueryResult:
    """æ”¹è¿›çš„æŸ¥è¯¢ç»“æœæ•°æ®ç±»"""
    query: str
    generated_answer: str           # RAGç”Ÿæˆçš„ç­”æ¡ˆ
    reference_answer: str           # æ ‡å‡†ç­”æ¡ˆ
    retrieved_docs: List[Document]
    semantic_similarities: List[float]  # æŸ¥è¯¢ä¸æ–‡æ¡£çš„è¯­ä¹‰ç›¸ä¼¼åº¦
    answer_similarity: float        # ç”Ÿæˆç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆçš„ç›¸ä¼¼åº¦
    response_time: float

@dataclass
class ImprovedChunkSizeResult:
    """æ”¹è¿›çš„chunk sizeæµ‹è¯•ç»“æœ"""
    chunk_size: int
    
    # æ ¸å¿ƒæŒ‡æ ‡
    avg_answer_similarity: float      # å¹³å‡ç­”æ¡ˆç›¸ä¼¼åº¦ï¼ˆä¸æ ‡å‡†ç­”æ¡ˆï¼‰
    avg_semantic_similarity: float    # å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆæŸ¥è¯¢ä¸æ–‡æ¡£ï¼‰
    avg_response_time: float         # å¹³å‡å“åº”æ—¶é—´
    
    # æ£€ç´¢è´¨é‡æŒ‡æ ‡
    avg_retrieval_relevance: float   # å¹³å‡æ£€ç´¢ç›¸å…³æ€§
    
    query_results: List[ImprovedQueryResult]

class ImprovedChunkSizeEvaluator:
    """æ”¹è¿›çš„Chunk Sizeè¯„ä¼°å™¨"""
    
    def __init__(self, document_path: str = "./data/1.txt", dataset_path: str = "./evaluation_dataset.json"):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.document_path = document_path
        self.dataset_path = dataset_path
        self.embedding = EmbeddingAdapter.get_embedding("dashscope", "text-embedding-v3")
        self.llm = LLMAdapter.get_llm("dashscope", "qwen-turbo", temperature=0.1)
        
        # åŠ è½½æ–‡æ¡£å†…å®¹
        self._load_document()
        
        # åŠ è½½è¯„ä¼°æ•°æ®é›†
        self._load_evaluation_dataset()
        
        print(f"âœ… æ”¹è¿›ç‰ˆè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“„ æ–‡æ¡£: {self.document_path}")
        print(f"ğŸ“‹ è¯„ä¼°æ•°æ®é›†: {self.dataset_path}")
        print(f"ğŸ“ æµ‹è¯•é—®é¢˜æ•°: {len(self.test_qa_pairs)}")
    
    def _load_document(self):
        """åŠ è½½æ–‡æ¡£å†…å®¹"""
        if not os.path.exists(self.document_path):
            raise FileNotFoundError(f"æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨: {self.document_path}")
        
        with open(self.document_path, 'r', encoding='utf-8') as f:
            self.document_content = f.read()
        
        print(f"ğŸ“š æ–‡æ¡£åŠ è½½æˆåŠŸï¼Œé•¿åº¦: {len(self.document_content)} å­—ç¬¦")
    
    def _load_evaluation_dataset(self):
        """åŠ è½½è¯„ä¼°æ•°æ®é›†"""
        if os.path.exists(self.dataset_path):
            # ä»æ–‡ä»¶åŠ è½½
            with open(self.dataset_path, 'r', encoding='utf-8') as f:
                self.test_qa_pairs = json.load(f)
            print(f"ğŸ“‹ ä» {self.dataset_path} åŠ è½½äº† {len(self.test_qa_pairs)} ä¸ªé—®ç­”å¯¹")
        else:
            # åˆ›å»ºé»˜è®¤ç¤ºä¾‹æ•°æ®é›†
            self._create_default_dataset()
            print(f"ğŸ“‹ åˆ›å»ºäº†é»˜è®¤è¯„ä¼°æ•°æ®é›†ï¼ŒåŒ…å« {len(self.test_qa_pairs)} ä¸ªé—®ç­”å¯¹")
            print(f"ğŸ’¡ æ‚¨å¯ä»¥ä¿®æ”¹ {self.dataset_path} æ–‡ä»¶æ¥è‡ªå®šä¹‰è¯„ä¼°æ•°æ®é›†")
    
    def _create_default_dataset(self):
        """åˆ›å»ºé»˜è®¤çš„è¯„ä¼°æ•°æ®é›†"""
        self.test_qa_pairs = [
            {
                "question": "ä¸€å¾®åŠå¯¼ä½“æ˜¯ä»€ä¹ˆå…¬å¸ï¼Ÿ",
                "reference_answer": "ä¸€å¾®åŠå¯¼ä½“æ˜¯ä¸€å®¶ä¸“ä¸šä»äº‹é›†æˆç”µè·¯è®¾è®¡ã€ç ”å‘å’Œé”€å”®çš„é«˜æ–°æŠ€æœ¯ä¼ä¸šï¼Œä¸»è¦ä¸šåŠ¡æ¶µç›–èŠ¯ç‰‡è®¾è®¡å’ŒåŠå¯¼ä½“ç›¸å…³äº§å“çš„å¼€å‘ã€‚"
            },
            {
                "question": "å‘˜å·¥è¿Ÿåˆ°ä¼šæœ‰ä»€ä¹ˆå¤„ç½šï¼Ÿ",
                "reference_answer": "å‘˜å·¥è¿Ÿåˆ°å°†æ ¹æ®è€ƒå‹¤åˆ¶åº¦è¿›è¡Œç›¸åº”å¤„ç½šï¼Œé€šå¸¸åŒ…æ‹¬å£å¤´è­¦å‘Šã€ä¹¦é¢è­¦å‘Šï¼Œä¸¥é‡è€…å¯èƒ½æ‰£é™¤ç›¸åº”å·¥èµ„æˆ–å¥–é‡‘ã€‚"
            },
            {
                "question": "å…¬å¸çš„æ ¸å¿ƒä»·å€¼è§‚æ˜¯ä»€ä¹ˆï¼Ÿ",
                "reference_answer": "å…¬å¸ç§‰æ‰¿åˆ›æ–°ã€è¯šä¿¡ã€å›¢é˜Ÿåˆä½œå’Œå®¢æˆ·è‡³ä¸Šçš„æ ¸å¿ƒä»·å€¼è§‚ï¼Œè‡´åŠ›äºä¸ºå®¢æˆ·æä¾›ä¼˜è´¨çš„äº§å“å’ŒæœåŠ¡ã€‚"
            },
            {
                "question": "å…¬å¸æœ‰å¤šå°‘å‘˜å·¥ï¼Ÿ",
                "reference_answer": "å…¬å¸ç›®å‰æ‹¥æœ‰æ•°ç™¾åå‘˜å·¥ï¼Œæ¶µç›–ç ”å‘ã€é”€å”®ã€å¸‚åœºã€è¡Œæ”¿ç­‰å„ä¸ªéƒ¨é—¨ã€‚"
            },
            {
                "question": "å…¬å¸çš„è€ƒå‹¤æ—¶é—´æ˜¯æ€æ ·çš„ï¼Ÿ",
                "reference_answer": "å…¬å¸å®è¡Œæ ‡å‡†å·¥ä½œæ—¶é—´åˆ¶ï¼Œé€šå¸¸ä¸ºå‘¨ä¸€è‡³å‘¨äº”ä¸Šåˆ9:00-12:00ï¼Œä¸‹åˆ13:30-18:00ï¼Œå…·ä½“æ—¶é—´å®‰æ’å¯èƒ½æ ¹æ®éƒ¨é—¨æœ‰æ‰€è°ƒæ•´ã€‚"
            }
        ]
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(self.dataset_path, 'w', encoding='utf-8') as f:
            json.dump(self.test_qa_pairs, f, ensure_ascii=False, indent=2)
    
    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
        try:
            # è·å–ä¸¤ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡
            embedding1 = self.embedding.embed_query(text1)
            embedding2 = self.embedding.embed_query(text2)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = cosine_similarity(
                np.array(embedding1).reshape(1, -1),
                np.array(embedding2).reshape(1, -1)
            )[0][0]
            
            return float(similarity)
        except Exception as e:
            print(f"âš ï¸ è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0
    
    def _evaluate_retrieval_relevance(self, query: str, retrieved_docs: List[Document]) -> float:
        """è¯„ä¼°æ£€ç´¢æ–‡æ¡£çš„ç›¸å…³æ€§"""
        if not retrieved_docs:
            return 0.0
        
        relevance_scores = []
        for doc in retrieved_docs:
            # è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£çš„è¯­ä¹‰ç›¸ä¼¼åº¦
            similarity = self._calculate_semantic_similarity(query, doc.page_content)
            relevance_scores.append(similarity)
        
        return np.mean(relevance_scores)
    
    def _evaluate_query_improved(self, vector_store: VectorStoreManager, qa_pair: Dict, k: int = 5) -> ImprovedQueryResult:
        """æ”¹è¿›çš„æŸ¥è¯¢è¯„ä¼°"""
        question = qa_pair["question"]
        reference_answer = qa_pair["reference_answer"]
        
        # æ‰§è¡Œæ£€ç´¢
        start_time = time.time()
        retriever = vector_store._create_retriever(
            search_type="similarity",
            search_kwargs={"k": k}
        )
        retrieved_docs = retriever.invoke(question)
        
        # ç”Ÿæˆç­”æ¡ˆ
        context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        answer_prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·ç»™å‡ºå‡†ç¡®ã€å®Œæ•´çš„ç­”æ¡ˆï¼š"""
        
        answer_response = self.llm.invoke(answer_prompt)
        generated_answer = answer_response.content if hasattr(answer_response, 'content') else str(answer_response)
        
        response_time = time.time() - start_time
        
        # è®¡ç®—æŸ¥è¯¢ä¸æ£€ç´¢æ–‡æ¡£çš„è¯­ä¹‰ç›¸ä¼¼åº¦
        semantic_similarities = []
        for doc in retrieved_docs:
            similarity = self._calculate_semantic_similarity(question, doc.page_content)
            semantic_similarities.append(similarity)
        
        # è®¡ç®—ç”Ÿæˆç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆçš„ç›¸ä¼¼åº¦
        answer_similarity = self._calculate_semantic_similarity(generated_answer, reference_answer)
        
        return ImprovedQueryResult(
            query=question,
            generated_answer=generated_answer,
            reference_answer=reference_answer,
            retrieved_docs=retrieved_docs,
            semantic_similarities=semantic_similarities,
            answer_similarity=answer_similarity,
            response_time=response_time
        )
    
    def test_chunk_size_improved(self, chunk_size: int, k: int = 5) -> ImprovedChunkSizeResult:
        """æ”¹è¿›çš„chunk sizeæµ‹è¯•"""
        print(f"\n{'='*60}")
        print(f"ğŸ§ª æµ‹è¯• Chunk Size: {chunk_size}")
        print(f"{'='*60}")
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        collection_name = f"improved_chunk_test_{chunk_size}"
        persist_dir = f"./improved_chunk_test_db_{chunk_size}"
        
        if os.path.exists(persist_dir):
            shutil.rmtree(persist_dir)
        
        vector_store = VectorStoreManager(
            embedding_model=self.embedding,
            collection_name=collection_name,
            persist_directory=persist_dir
        )
        
        document = Document(
            page_content=self.document_content,
            metadata={'source': self.document_path}
        )
        
        vector_store.create_from_documents(
            [document],
            chunk_size=chunk_size,
            chunk_overlap=100
        )
        
        print(f"ğŸ“¦ åˆ›å»ºäº† {len(vector_store)} ä¸ªæ–‡æ¡£å—")
        
        # æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
        query_results = []
        
        for i, qa_pair in enumerate(self.test_qa_pairs, 1):
            print(f"ğŸ“‹ é—®é¢˜ {i}/{len(self.test_qa_pairs)}: {qa_pair['question']}")
            
            result = self._evaluate_query_improved(vector_store, qa_pair, k)
            query_results.append(result)
            
            print(f"   â±ï¸ å“åº”æ—¶é—´: {result.response_time:.3f}s")
            print(f"   ğŸ“Š å¹³å‡æ£€ç´¢ç›¸ä¼¼åº¦: {np.mean(result.semantic_similarities):.3f}")
            print(f"   ğŸ¯ ç­”æ¡ˆç›¸ä¼¼åº¦: {result.answer_similarity:.3f}")
            print(f"   ğŸ’¬ ç”Ÿæˆç­”æ¡ˆ: {result.generated_answer[:50]}...")
        
        # è®¡ç®—èšåˆæŒ‡æ ‡
        avg_answer_similarity = np.mean([r.answer_similarity for r in query_results])
        avg_semantic_similarity = np.mean([np.mean(r.semantic_similarities) for r in query_results])
        avg_response_time = np.mean([r.response_time for r in query_results])
        
        # è®¡ç®—æ£€ç´¢è´¨é‡æŒ‡æ ‡
        retrieval_relevance_scores = []
        
        for result in query_results:
            # æ£€ç´¢ç›¸å…³æ€§
            relevance = self._evaluate_retrieval_relevance(result.query, result.retrieved_docs)
            retrieval_relevance_scores.append(relevance)
        
        avg_retrieval_relevance = np.mean(retrieval_relevance_scores)
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        shutil.rmtree(persist_dir, ignore_errors=True)
        
        print(f"\nğŸ“Š Chunk Size {chunk_size} æµ‹è¯•ç»“æœ:")
        print(f"   ğŸ¯ å¹³å‡ç­”æ¡ˆç›¸ä¼¼åº¦: {avg_answer_similarity:.3f}")
        print(f"   ğŸ” å¹³å‡æ£€ç´¢ç›¸ä¼¼åº¦: {avg_semantic_similarity:.3f}")
        print(f"   ğŸ“‹ å¹³å‡æ£€ç´¢ç›¸å…³æ€§: {avg_retrieval_relevance:.3f}")
        print(f"   â±ï¸ å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}s")
        
        return ImprovedChunkSizeResult(
            chunk_size=chunk_size,
            avg_answer_similarity=avg_answer_similarity,
            avg_semantic_similarity=avg_semantic_similarity,
            avg_response_time=avg_response_time,
            avg_retrieval_relevance=avg_retrieval_relevance,
            query_results=query_results
        )
    
    def comprehensive_evaluation(self, chunk_sizes: List[int] = None, k: int = 5) -> Dict[str, Any]:
        """ç»¼åˆè¯„ä¼°å¤šä¸ªchunk size"""
        if chunk_sizes is None:
            chunk_sizes = [256, 512, 800, 1024, 1500, 2048]
        
        print(f"\n" + "="*80)
        print(f"ğŸ§ª Chunk Size ç»¼åˆè¯„ä¼° - åŸºäºäººå·¥æ ‡æ³¨æ•°æ®é›†")
        print(f"="*80)
        print(f"ğŸ“‹ æµ‹è¯•é…ç½®:")
        print(f"   ğŸ“„ æ–‡æ¡£: {self.document_path}")
        print(f"   ğŸ“ è¯„ä¼°æ•°æ®é›†: {self.dataset_path}")
        print(f"   ğŸ” æ£€ç´¢æ•°é‡ (k): {k}")
        print(f"   ğŸ“ æµ‹è¯•é—®é¢˜æ•°: {len(self.test_qa_pairs)}")
        print(f"   ğŸ›ï¸ Chunk Sizes: {chunk_sizes}")
        
        results = []
        
        # æµ‹è¯•æ¯ä¸ªchunk size
        for chunk_size in chunk_sizes:
            try:
                result = self.test_chunk_size_improved(chunk_size, k)
                results.append(result)
            except Exception as e:
                print(f"âŒ Chunk Size {chunk_size} æµ‹è¯•å¤±è´¥: {e}")
        
        # åˆ†æç»“æœ
        analysis = self._analyze_results(results)
        
        # ä¿å­˜ç»“æœ
        self._save_results(results, analysis)
        
        return {
            "results": results,
            "analysis": analysis
        }
    
    def _analyze_results(self, results: List[ImprovedChunkSizeResult]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        if not results:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•ç»“æœ"}
        
        print(f"\n" + "="*80)
        print(f"ğŸ“Š ç»“æœåˆ†æ")
        print(f"="*80)
        
        # æŒ‰ä¸åŒæŒ‡æ ‡æ’åº
        by_answer_similarity = sorted(results, key=lambda x: x.avg_answer_similarity, reverse=True)
        by_retrieval_relevance = sorted(results, key=lambda x: x.avg_retrieval_relevance, reverse=True)
        by_speed = sorted(results, key=lambda x: x.avg_response_time)
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        def calculate_composite_score(result: ImprovedChunkSizeResult) -> float:
            # å½’ä¸€åŒ–å„æŒ‡æ ‡
            max_answer_sim = max(r.avg_answer_similarity for r in results)
            max_retrieval_rel = max(r.avg_retrieval_relevance for r in results)
            min_time = min(r.avg_response_time for r in results)
            
            norm_answer_sim = result.avg_answer_similarity / max_answer_sim if max_answer_sim > 0 else 0
            norm_retrieval_rel = result.avg_retrieval_relevance / max_retrieval_rel if max_retrieval_rel > 0 else 0
            norm_speed = min_time / result.avg_response_time if result.avg_response_time > 0 else 0
            
            # åŠ æƒè®¡ç®—ç»¼åˆè¯„åˆ†
            composite = (norm_answer_sim * 0.5 +      # ç­”æ¡ˆç›¸ä¼¼åº¦æƒé‡ 50%
                        norm_retrieval_rel * 0.25 +   # æ£€ç´¢ç›¸å…³æ€§æƒé‡ 25%
                        norm_speed * 0.25)             # é€Ÿåº¦æƒé‡ 25%
            
            return composite
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        for result in results:
            result.composite_score = calculate_composite_score(result)
        
        by_composite = sorted(results, key=lambda x: x.composite_score, reverse=True)
        
        # æ‰“å°è¯¦ç»†å¯¹æ¯”è¡¨
        print(f"\nğŸ“‹ è¯¦ç»†å¯¹æ¯”è¡¨:")
        print(f"{'Chunk Size':<11} {'ç­”æ¡ˆç›¸ä¼¼åº¦':<10} {'æ£€ç´¢ç›¸å…³æ€§':<10} {'å“åº”æ—¶é—´':<10} {'ç»¼åˆè¯„åˆ†':<8}")
        print("-" * 75)
        
        for result in results:
            print(f"{result.chunk_size:<11} "
                  f"{result.avg_answer_similarity:<10.3f} "
                  f"{result.avg_retrieval_relevance:<10.3f} "
                  f"{result.avg_response_time:<10.3f} "
                  f"{result.composite_score:<8.3f}")
        
        # æœ€ä½³é…ç½®åˆ†æ
        print(f"\nğŸ† æœ€ä½³é…ç½®åˆ†æ:")
        print(f"   ğŸ¯ æœ€é«˜ç­”æ¡ˆç›¸ä¼¼åº¦: Chunk Size {by_answer_similarity[0].chunk_size} (ç›¸ä¼¼åº¦: {by_answer_similarity[0].avg_answer_similarity:.3f})")
        print(f"   ğŸ” æœ€é«˜æ£€ç´¢ç›¸å…³æ€§: Chunk Size {by_retrieval_relevance[0].chunk_size} (ç›¸å…³æ€§: {by_retrieval_relevance[0].avg_retrieval_relevance:.3f})")
        print(f"   ğŸš€ æœ€å¿«å“åº”: Chunk Size {by_speed[0].chunk_size} (æ—¶é—´: {by_speed[0].avg_response_time:.3f}s)")
        print(f"   ğŸ–ï¸ æœ€ä½³ç»¼åˆ: Chunk Size {by_composite[0].chunk_size} (ç»¼åˆè¯„åˆ†: {by_composite[0].composite_score:.3f})")
        
        # æ¨èé…ç½®
        recommended = by_composite[0]
        
        print(f"\nğŸ’¡ æ¨èé…ç½®:")
        print(f"   ğŸ“ å»ºè®®ä½¿ç”¨ Chunk Size: {recommended.chunk_size}")
        print(f"   ğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        print(f"      ğŸ¯ ç­”æ¡ˆç›¸ä¼¼åº¦: {recommended.avg_answer_similarity:.3f}")
        print(f"      ğŸ” æ£€ç´¢ç›¸å…³æ€§: {recommended.avg_retrieval_relevance:.3f}")
        print(f"      â±ï¸ å“åº”æ—¶é—´: {recommended.avg_response_time:.3f}s")
        print(f"      ğŸ–ï¸ ç»¼åˆè¯„åˆ†: {recommended.composite_score:.3f}")
        
        print(f"\nğŸ”§ ä½¿ç”¨å»ºè®®:")
        print(f"   - åœ¨RAGç³»ç»Ÿä¸­è®¾ç½® chunk_size={recommended.chunk_size}")
        print(f"   - è¯¥é…ç½®åœ¨ç­”æ¡ˆè´¨é‡å’Œæ£€ç´¢æ•ˆæœä¹‹é—´å–å¾—äº†æœ€ä½³å¹³è¡¡")
        
        if recommended.chunk_size != by_answer_similarity[0].chunk_size:
            print(f"   - å¦‚æœæ›´é‡è§†ç­”æ¡ˆè´¨é‡ï¼Œå¯è€ƒè™‘ chunk_size={by_answer_similarity[0].chunk_size}")
        if recommended.chunk_size != by_speed[0].chunk_size:
            print(f"   - å¦‚æœæ›´é‡è§†å“åº”é€Ÿåº¦ï¼Œå¯è€ƒè™‘ chunk_size={by_speed[0].chunk_size}")
        
        return {
            "best_overall": {
                "chunk_size": recommended.chunk_size,
                "metrics": {
                    "answer_similarity": recommended.avg_answer_similarity,
                    "retrieval_relevance": recommended.avg_retrieval_relevance,
                    "response_time": recommended.avg_response_time,
                    "composite_score": recommended.composite_score
                }
            },
            "best_by_metric": {
                "answer_similarity": {"chunk_size": by_answer_similarity[0].chunk_size, "value": by_answer_similarity[0].avg_answer_similarity},
                "retrieval_relevance": {"chunk_size": by_retrieval_relevance[0].chunk_size, "value": by_retrieval_relevance[0].avg_retrieval_relevance},
                "speed": {"chunk_size": by_speed[0].chunk_size, "value": by_speed[0].avg_response_time}
            },
            "all_results": [
                {
                    "chunk_size": r.chunk_size,
                    "answer_similarity": r.avg_answer_similarity,
                    "retrieval_relevance": r.avg_retrieval_relevance,
                    "response_time": r.avg_response_time,
                    "composite_score": r.composite_score
                }
                for r in results
            ]
        }
    
    def _save_results(self, results: List[ImprovedChunkSizeResult], analysis: Dict[str, Any]):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"chunk_size_evaluation_{timestamp}.json"
        
        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        save_data = {
            "timestamp": timestamp,
            "document": self.document_path,
            "dataset": self.dataset_path,
            "test_questions_count": len(self.test_qa_pairs),
            "results": [
                {
                    "chunk_size": r.chunk_size,
                    "answer_similarity": r.avg_answer_similarity,
                    "retrieval_relevance": r.avg_retrieval_relevance,
                    "response_time": r.avg_response_time,
                    "composite_score": getattr(r, 'composite_score', 0),
                    "detailed_results": [
                        {
                            "question": qr.query,
                            "generated_answer": qr.generated_answer,
                            "reference_answer": qr.reference_answer,
                            "answer_similarity": qr.answer_similarity,
                            "response_time": qr.response_time
                        }
                        for qr in r.query_results
                    ]
                }
                for r in results
            ],
            "analysis": analysis
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {filename}")

def main():
    """ä¸»å‡½æ•°"""
    try:
        document_path = "./data/1.txt"
        dataset_path = "./evaluation_dataset.json"
        
        if not os.path.exists(document_path):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {document_path}")
            return
        
        evaluator = ImprovedChunkSizeEvaluator(document_path, dataset_path)
        
        # æ‰§è¡Œç»¼åˆè¯„ä¼°
        chunk_sizes = [256, 512, 800, 1024, 1500, 2048]
        evaluation_results = evaluator.comprehensive_evaluation(chunk_sizes, k=5)
        
        print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼")
        print(f"å»ºè®®çš„æœ€ä½³ Chunk Size: {evaluation_results['analysis']['best_overall']['chunk_size']}")
        
    except KeyboardInterrupt:
        print(f"\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œè¯„ä¼°åœæ­¢")
    except Exception as e:
        print(f"âŒ è¯„ä¼°å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 