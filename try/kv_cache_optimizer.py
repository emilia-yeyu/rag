#!/usr/bin/env python3
"""
KV Cache ä¼˜åŒ–æ¨¡å—
åŸºäº turboRAG æ€è·¯ï¼Œä¸ºç°æœ‰ RAG ç³»ç»Ÿæ·»åŠ  KV ç¼“å­˜ä¼˜åŒ–
"""

import os
import torch
import pickle
import hashlib
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
import time
from transformers import AutoTokenizer, AutoModelForCausalLM

# å¯¼å…¥ç°æœ‰ç»„ä»¶
from vector_store.vector_store import VectorStoreManager
from embedding.adapter import EmbeddingAdapter
from llm.adapter import LLMAdapter


class KVCacheManager:
    """KVç¼“å­˜ç®¡ç†å™¨ - æ ¸å¿ƒä¼˜åŒ–ç»„ä»¶"""
    
    def __init__(self, 
                 cache_dir: str = "./kv_cache",
                 model_name: str = None,
                 device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        """
        åˆå§‹åŒ–KVç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache_dir: ç¼“å­˜æ–‡ä»¶å­˜å‚¨ç›®å½•
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚æœä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰
            device: è®¡ç®—è®¾å¤‡
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.device = device
        
        # å¦‚æœæä¾›äº†æ¨¡å‹åç§°ï¼Œåˆå§‹åŒ–æœ¬åœ°æ¨¡å‹
        self.model = None
        self.tokenizer = None
        if model_name:
            self._init_local_model(model_name)
        
        # ç¼“å­˜ç´¢å¼•ï¼š{chunk_hash: cache_file_path}
        self.cache_index = self._load_cache_index()
        
        # é¢„å®šä¹‰çš„æç¤ºå‰ç¼€
        self.prefix_template = """ä½ æ˜¯ä¸€å¾®åŠå¯¼ä½“å…¬å¸çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
"""
        
        print(f"ğŸš€ KVç¼“å­˜ç®¡ç†å™¨å·²åˆå§‹åŒ–")
        print(f"ğŸ“ ç¼“å­˜ç›®å½•: {self.cache_dir}")
        print(f"ğŸ’¾ å·²æœ‰ç¼“å­˜æ•°é‡: {len(self.cache_index)}")
    
    def _init_local_model(self, model_name: str):
        """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
            print(f"âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
        except Exception as e:
            print(f"âš ï¸ æœ¬åœ°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ å°†ä½¿ç”¨è¿œç¨‹APIæ¨¡å¼")
    
    def _load_cache_index(self) -> Dict[str, str]:
        """åŠ è½½ç¼“å­˜ç´¢å¼•"""
        index_file = self.cache_dir / "cache_index.pkl"
        if index_file.exists():
            try:
                with open(index_file, 'rb') as f:
                    return pickle.load(f)
            except:
                pass
        return {}
    
    def _save_cache_index(self):
        """ä¿å­˜ç¼“å­˜ç´¢å¼•"""
        index_file = self.cache_dir / "cache_index.pkl"
        with open(index_file, 'wb') as f:
            pickle.dump(self.cache_index, f)
    
    def _get_chunk_hash(self, chunk_content: str) -> str:
        """è·å–æ–‡æ¡£å—çš„å“ˆå¸Œå€¼"""
        return hashlib.md5(chunk_content.encode('utf-8')).hexdigest()
    
    def preprocess_documents(self, vector_store: VectorStoreManager, 
                           batch_size: int = 8) -> int:
        """
        é¢„å¤„ç†æ–‡æ¡£ï¼Œç”ŸæˆKVç¼“å­˜
        
        Args:
            vector_store: å‘é‡å­˜å‚¨ç®¡ç†å™¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            int: å¤„ç†çš„æ–‡æ¡£å—æ•°é‡
        """
        print(f"ğŸ”„ å¼€å§‹é¢„å¤„ç†æ–‡æ¡£ï¼Œç”ŸæˆKVç¼“å­˜...")
        
        # è·å–æ‰€æœ‰æ–‡æ¡£å—
        try:
            documents = vector_store.get_all_documents()
            all_docs = []
            for doc in documents:
                all_docs.append({
                    'content': doc.page_content,
                    'metadata': doc.metadata
                })
        except Exception as e:
            print(f"âš ï¸ è·å–æ–‡æ¡£å¤±è´¥: {e}")
            all_docs = []
        
        print(f"ğŸ“„ å‘ç° {len(all_docs)} ä¸ªæ–‡æ¡£å—")
        
        processed_count = 0
        new_cache_count = 0
        
        for i, doc in enumerate(all_docs):
            content = doc['content']
            chunk_hash = self._get_chunk_hash(content)
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜
            if chunk_hash in self.cache_index:
                print(f"âš¡ è·³è¿‡å·²ç¼“å­˜å— {i+1}/{len(all_docs)}")
                processed_count += 1
                continue
            
            # ç”Ÿæˆå¸¦å‰ç¼€çš„å®Œæ•´å†…å®¹
            full_content = self.prefix_template + content
            
            # ç”ŸæˆKVç¼“å­˜
            cache_data = self._generate_kv_cache(full_content, chunk_hash)
            
            if cache_data:
                # ä¿å­˜ç¼“å­˜æ–‡ä»¶
                cache_file = self.cache_dir / f"cache_{chunk_hash}.pkl"
                with open(cache_file, 'wb') as f:
                    pickle.dump(cache_data, f)
                
                # æ›´æ–°ç´¢å¼•
                self.cache_index[chunk_hash] = str(cache_file)
                new_cache_count += 1
                
                print(f"ğŸ’¾ å·²å¤„ç†å— {i+1}/{len(all_docs)} (æ–°å¢ç¼“å­˜)")
            
            processed_count += 1
        
        # ä¿å­˜ç´¢å¼•
        self._save_cache_index()
        
        print(f"âœ… æ–‡æ¡£é¢„å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“Š æ€»å¤„ç†: {processed_count} å—")
        print(f"ğŸ†• æ–°å¢ç¼“å­˜: {new_cache_count} å—")
        print(f"ğŸ’¾ æ€»ç¼“å­˜æ•°: {len(self.cache_index)} å—")
        
        return processed_count
    
    def _generate_kv_cache(self, content: str, chunk_hash: str) -> Optional[Dict[str, Any]]:
        """
        ä¸ºå•ä¸ªæ–‡æ¡£å—ç”ŸæˆKVç¼“å­˜
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            chunk_hash: æ–‡æ¡£å“ˆå¸Œ
            
        Returns:
            Optional[Dict]: ç¼“å­˜æ•°æ®æˆ–None
        """
        if self.model is None or self.tokenizer is None:
            # å¦‚æœæ²¡æœ‰æœ¬åœ°æ¨¡å‹ï¼Œä¿å­˜æ–‡æœ¬ä¿¡æ¯ä¾›åç»­ä¼˜åŒ–
            return {
                'content': content,
                'chunk_hash': chunk_hash,
                'type': 'text_cache',
                'timestamp': time.time()
            }
        
        try:
            # ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”ŸæˆKVç¼“å­˜
            inputs = self.tokenizer(
                content, 
                return_tensors="pt",
                truncation=True,
                max_length=2048
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model(**inputs, use_cache=True)
            
            # ä¿å­˜KVç¼“å­˜åˆ°CPUä»¥èŠ‚çœæ˜¾å­˜
            kv_cache = None
            if hasattr(outputs, 'past_key_values') and outputs.past_key_values:
                kv_cache = []
                for layer_cache in outputs.past_key_values:
                    kv_cache.append(tuple(tensor.cpu() for tensor in layer_cache))
            
            return {
                'content': content,
                'chunk_hash': chunk_hash,
                'kv_cache': kv_cache,
                'input_ids': inputs['input_ids'].cpu(),
                'attention_mask': inputs['attention_mask'].cpu(),
                'type': 'full_cache',
                'timestamp': time.time()
            }
            
        except Exception as e:
            print(f"âš ï¸ ç”ŸæˆKVç¼“å­˜å¤±è´¥ {chunk_hash}: {e}")
            return None
    
    def get_cached_chunks(self, chunk_contents: List[str]) -> Tuple[List[Dict], List[str]]:
        """
        è·å–æ–‡æ¡£å—çš„ç¼“å­˜ä¿¡æ¯
        
        Args:
            chunk_contents: æ–‡æ¡£å—å†…å®¹åˆ—è¡¨
            
        Returns:
            Tuple[List[Dict], List[str]]: (ç¼“å­˜æ•°æ®åˆ—è¡¨, æœªç¼“å­˜å†…å®¹åˆ—è¡¨)
        """
        cached_data = []
        uncached_contents = []
        
        for content in chunk_contents:
            chunk_hash = self._get_chunk_hash(content)
            
            if chunk_hash in self.cache_index:
                cache_file = self.cache_index[chunk_hash]
                try:
                    with open(cache_file, 'rb') as f:
                        cache_data = pickle.load(f)
                    cached_data.append(cache_data)
                except:
                    # ç¼“å­˜æ–‡ä»¶æŸåï¼Œæ·»åŠ åˆ°æœªç¼“å­˜åˆ—è¡¨
                    uncached_contents.append(content)
            else:
                uncached_contents.append(content)
        
        return cached_data, uncached_contents
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_size = 0
        cache_files = list(self.cache_dir.glob("cache_*.pkl"))
        
        for cache_file in cache_files:
            total_size += cache_file.stat().st_size
        
        return {
            'total_caches': len(self.cache_index),
            'cache_files': len(cache_files),
            'total_size_mb': total_size / (1024 * 1024),
            'cache_directory': str(self.cache_dir)
        }
    
    def clear_cache(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        import shutil
        if self.cache_dir.exists():
            shutil.rmtree(self.cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_index = {}
        print(f"ğŸ—‘ï¸ å·²æ¸…ç†æ‰€æœ‰KVç¼“å­˜")


class OptimizedRAG:
    """ä¼˜åŒ–ç‰ˆRAG - é›†æˆKVç¼“å­˜"""
    
    def __init__(self, original_rag, cache_manager: KVCacheManager):
        """
        åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆRAG
        
        Args:
            original_rag: åŸå§‹RAGç³»ç»Ÿå®ä¾‹
            cache_manager: KVç¼“å­˜ç®¡ç†å™¨
        """
        self.original_rag = original_rag
        self.cache_manager = cache_manager
        
        print(f"ğŸš€ ä¼˜åŒ–ç‰ˆRAGå·²åˆå§‹åŒ–")
    
    def preprocess_knowledge_base(self):
        """é¢„å¤„ç†çŸ¥è¯†åº“ï¼Œç”ŸæˆKVç¼“å­˜"""
        return self.cache_manager.preprocess_documents(
            self.original_rag.vector_store
        )
    
    def optimized_query(self, question: str, show_sources: bool = False) -> Dict[str, Any]:
        """
        ä¼˜åŒ–çš„æŸ¥è¯¢æ–¹æ³•
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            show_sources: æ˜¯å¦æ˜¾ç¤ºæ¥æº
            
        Returns:
            Dict[str, Any]: æŸ¥è¯¢ç»“æœ
        """
        print(f"ğŸ” ä¼˜åŒ–æŸ¥è¯¢: {question}")
        start_time = time.time()
        
        try:
            # 1. å‘é‡æ£€ç´¢ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            retriever = self.original_rag.vector_store._create_retriever(
                search_type="similarity",
                search_kwargs={"k": 5}
            )
            retrieved_docs = retriever.invoke(question)
            
            retrieval_time = time.time() - start_time
            print(f"ğŸ“š æ£€ç´¢å®Œæˆï¼Œè€—æ—¶: {retrieval_time:.2f}ç§’")
            
            # 2. è·å–ç¼“å­˜ä¿¡æ¯
            chunk_contents = [doc.page_content for doc in retrieved_docs]
            cached_data, uncached_contents = self.cache_manager.get_cached_chunks(chunk_contents)
            
            cache_hit_rate = len(cached_data) / len(chunk_contents) if chunk_contents else 0
            print(f"âš¡ ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1%} ({len(cached_data)}/{len(chunk_contents)})")
            
            # 3. å¦‚æœæœ‰ç¼“å­˜ï¼Œä½¿ç”¨ä¼˜åŒ–è·¯å¾„
            if cached_data:
                # ä½¿ç”¨ç¼“å­˜è¿›è¡Œå¿«é€Ÿæ¨ç†
                answer = self._fast_inference_with_cache(question, cached_data, uncached_contents)
            else:
                # å›é€€åˆ°åŸå§‹æ–¹æ³•
                print(f"âš ï¸ æ— å¯ç”¨ç¼“å­˜ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•")
                original_result = self.original_rag.query(question, show_sources)
                answer = original_result['answer']
            
            # 4. å‡†å¤‡ç»“æœ
            result = {
                "question": question,
                "answer": answer,
                "response_time": f"{time.time() - start_time:.2f}ç§’",
                "cache_hit_rate": f"{cache_hit_rate:.1%}",
                "optimization_used": len(cached_data) > 0,
                "sources": [{"content": doc.page_content[:200] + "...", 
                           "metadata": doc.metadata} 
                          for doc in retrieved_docs] if show_sources else []
            }
            
            print(f"âœ… ä¼˜åŒ–æŸ¥è¯¢å®Œæˆï¼Œæ€»è€—æ—¶: {result['response_time']}")
            return result
            
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–æŸ¥è¯¢å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•: {e}")
            return self.original_rag.query(question, show_sources)
    
    def _fast_inference_with_cache(self, question: str, 
                                 cached_data: List[Dict], 
                                 uncached_contents: List[str]) -> str:
        """
        ä½¿ç”¨ç¼“å­˜è¿›è¡Œå¿«é€Ÿæ¨ç†
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            cached_data: ç¼“å­˜æ•°æ®
            uncached_contents: æœªç¼“å­˜å†…å®¹
            
        Returns:
            str: ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        # ç»„è£…ä¸Šä¸‹æ–‡
        context_parts = []
        
        # æ·»åŠ ç¼“å­˜çš„å†…å®¹
        for cache_item in cached_data:
            if cache_item.get('type') == 'full_cache':
                # å¦‚æœæœ‰å®Œæ•´KVç¼“å­˜ï¼Œæå–å†…å®¹
                content = cache_item['content']
                # ç§»é™¤å‰ç¼€éƒ¨åˆ†ï¼Œåªä¿ç•™æ–‡æ¡£å†…å®¹
                if content.startswith(self.cache_manager.prefix_template):
                    content = content[len(self.cache_manager.prefix_template):]
                context_parts.append(content)
            else:
                # æ–‡æœ¬ç¼“å­˜
                content = cache_item['content']
                if content.startswith(self.cache_manager.prefix_template):
                    content = content[len(self.cache_manager.prefix_template):]
                context_parts.append(content)
        
        # æ·»åŠ æœªç¼“å­˜çš„å†…å®¹
        context_parts.extend(uncached_contents)
        
        # ç»„è£…å®Œæ•´æç¤º
        context = "\n\n".join(context_parts)
        prompt = f"""ä½ æ˜¯ä¸€å¾®åŠå¯¼ä½“å…¬å¸çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åŸºäºæ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯è¯·è¯´æ˜ã€‚

å›ç­”ï¼š"""
        
        # ä½¿ç”¨åŸå§‹LLMè¿›è¡Œæ¨ç†ï¼ˆè¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰
        try:
            answer = self.original_rag.llm.invoke(prompt)
            return answer
        except Exception as e:
            print(f"âš ï¸ å¿«é€Ÿæ¨ç†å¤±è´¥: {e}")
            # å›é€€åˆ°ç»„è£…RAGé“¾
            return self._fallback_to_rag_chain(question, context_parts)
    
    def _fallback_to_rag_chain(self, question: str, context_parts: List[str]) -> str:
        """å›é€€åˆ°åŸå§‹RAGé“¾æ–¹æ³•"""
        try:
            # é‡æ–°ç»„è£…ä¸ºDocumentå¯¹è±¡
            from langchain_core.documents import Document
            docs = [Document(page_content=content) for content in context_parts]
            
            # ä½¿ç”¨åŸå§‹RAGé“¾
            context = "\n\n".join(doc.page_content for doc in docs)
            
            from langchain_core.prompts import PromptTemplate
            from langchain_core.output_parsers import StrOutputParser
            
            prompt = PromptTemplate(
                template="""ä½ æ˜¯ä¸€å¾®åŠå¯¼ä½“å…¬å¸çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åŸºäºæ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ï¼Œå¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯è¯·è¯´æ˜ã€‚

å›ç­”ï¼š""",
                input_variables=["context", "question"]
            )
            
            chain = prompt | self.original_rag.llm | StrOutputParser()
            return chain.invoke({"context": context, "question": question})
            
        except Exception as e:
            print(f"âŒ å›é€€æ–¹æ³•ä¹Ÿå¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œå¤„ç†é—®é¢˜æ—¶å‡ºç°é”™è¯¯: {str(e)}"


def create_optimized_rag(original_rag, model_name: str = None) -> OptimizedRAG:
    """
    åˆ›å»ºä¼˜åŒ–ç‰ˆRAGç³»ç»Ÿçš„å·¥å‚å‡½æ•°
    
    Args:
        original_rag: åŸå§‹RAGç³»ç»Ÿå®ä¾‹
        model_name: å¯é€‰çš„æœ¬åœ°æ¨¡å‹åç§°
        
    Returns:
        OptimizedRAG: ä¼˜åŒ–ç‰ˆRAGç³»ç»Ÿ
    """
    cache_manager = KVCacheManager(model_name=model_name)
    return OptimizedRAG(original_rag, cache_manager)


if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å—
    print("ğŸ§ª æµ‹è¯•KVç¼“å­˜ä¼˜åŒ–æ¨¡å—")
    print("=" * 50)
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•ä»£ç 
    cache_manager = KVCacheManager()
    stats = cache_manager.get_cache_stats()
    
    print(f"ğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
    for key, value in stats.items():
        print(f"  {key}: {value}") 