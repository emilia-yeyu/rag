# Nano GraphRAG 项目说明文档

## 项目概述

Nano GraphRAG是一个基于知识图谱的检索增强生成(Retrieval-Augmented Generation)系统的轻量级实现。它通过构建实体关系图谱、社区检测和多模式检索来增强大语言模型的查询能力。该项目参考了微软的GraphRAG实现，但进行了精简和优化。

## 项目架构图

```
nano_graphrag/
├── graphrag.py          # 主入口类
├── base.py             # 基础接口定义  
├── _op.py              # 核心操作模块
├── _llm.py             # LLM接口模块
├── _utils.py           # 工具函数模块
├── prompt.py           # 提示词模板
├── _splitter.py        # 文本分割器
├── _storage/           # 存储层实现
│   ├── __init__.py
│   ├── kv_json.py      # JSON键值存储
│   ├── vdb_nanovectordb.py  # 向量数据库
│   └── gdb_neo4j.py    # Neo4j图数据库
├── entity_extraction/ # 实体提取模块
│   ├── __init__.py
│   ├── module.py      # 核心实体关系抽取器
│   ├── extract.py     # 提取流程实现
│   └── metric.py      # 评估指标
└── examples/          # 示例代码
    ├── simple.py
    ├── use_local_embedding.py
    └── ...
```

## 核心文件详细分析

### 1. graphrag.py - 主入口类 (13KB, 383行)

这是整个系统的核心入口，定义了`GraphRAG`类，负责协调所有组件。

#### 核心功能:
```python
@dataclass
class GraphRAG:
    # 文本处理配置
    chunk_token_size: int = 1200
    chunk_overlap_token_size: int = 100
    
    # 实体提取配置
    entity_extract_max_gleaning: int = 1
    entity_summary_to_max_tokens: int = 500
    
    # 图聚类配置
    graph_cluster_algorithm: str = "leiden"
    max_graph_cluster_size: int = 10
    
    # 嵌入和LLM配置
    embedding_func: EmbeddingFunc = openai_embedding
    best_model_func: callable = gpt_4o_complete
    cheap_model_func: callable = gpt_4o_mini_complete
```

#### 关键方法:
- `insert()`: 文档插入流程
- `query()`: 查询接口，支持local/global/naive三种模式
- `ainsert()`: 异步插入实现
- `aquery()`: 异步查询实现

### 2. _op.py - 核心操作模块 (39KB, 1140行)

这是系统最重要的模块，包含了所有核心算法实现。

#### 主要功能模块:

##### 文本分块处理:
```python
def chunking_by_token_size(tokens_list, doc_keys, tokenizer_wrapper, 
                          overlap_token_size=128, max_token_size=1024):
    # 按token大小分块，支持重叠
```

##### 实体关系提取:
```python
async def extract_entities(chunks, knwoledge_graph_inst, entity_vdb, 
                          tokenizer_wrapper, global_config):
    # 使用LLM提取实体和关系
    # 支持多轮gleaning提取
    # 实体去重和合并
```

##### 社区报告生成:
```python
async def generate_community_report(community_report_kv, knwoledge_graph_inst, 
                                   tokenizer_wrapper, global_config):
    # 基于图聚类结果生成社区报告
    # 分层处理，支持子社区
```

#### 核心图检索算法:

##### Local查询 (重点):
```python
async def local_query(query, knowledge_graph_inst, entities_vdb, 
                     community_reports, text_chunks_db, query_param, 
                     tokenizer_wrapper, global_config):
    # 1. 向量检索相关实体
    results = await entities_vdb.query(query, top_k=query_param.top_k)
    
    # 2. 获取实体邻居和度数
    node_datas = await knowledge_graph_inst.get_nodes_batch([r["entity_name"] for r in results])
    node_degrees = await knowledge_graph_inst.node_degrees_batch([r["entity_name"] for r in results])
    
    # 3. 找到最相关的社区报告
    use_communities = await _find_most_related_community_from_entities(...)
    
    # 4. 找到相关的文本单元
    use_text_units = await _find_most_related_text_unit_from_entities(...)
    
    # 5. 找到相关的边关系
    use_relations = await _find_most_related_edges_from_entities(...)
    
    # 6. 构建上下文并生成回答
    context = _build_local_query_context(...)
    response = await use_model_func(query, system_prompt=sys_prompt)
```

**Local查询的核心原理:**
1. **实体检索**: 使用向量相似度找到与查询最相关的实体
2. **图扩展**: 获取这些实体的邻居节点和边，基于度数排序
3. **社区聚合**: 找到包含这些实体的社区及其报告
4. **上下文构建**: 将实体、关系、社区报告、原文片段组合成结构化上下文
5. **LLM生成**: 基于丰富的图结构上下文生成回答

##### Global查询:
```python
async def global_query(query, knowledge_graph_inst, entities_vdb, 
                      community_reports, text_chunks_db, query_param, 
                      tokenizer_wrapper, global_config):
    # 1. 获取所有社区schema
    community_schema = await knowledge_graph_inst.community_schema()
    
    # 2. 按重要性排序社区
    sorted_community_schemas = sorted(community_schema.items(), 
                                    key=lambda x: x[1]["occurrence"], reverse=True)
    
    # 3. Map阶段: 分组处理社区报告
    map_communities_points = await _map_global_communities(...)
    
    # 4. Reduce阶段: 合并所有分析要点
    final_support_points = []
    for i, mc in enumerate(map_communities_points):
        for point in mc:
            final_support_points.append({
                "analyst": i,
                "answer": point["description"], 
                "score": point.get("score", 1)
            })
    
    # 5. 基于合并的要点生成最终回答
    response = await use_model_func(query, sys_prompt_temp.format(...))
```

**Global查询的Map-Reduce模式:**
- **Map阶段**: 将社区分组，每组独立分析生成要点
- **Reduce阶段**: 合并所有要点，按重要性排序，生成综合回答

##### Naive查询:
```python
async def naive_query(query, chunks_vdb, text_chunks_db, query_param, 
                     tokenizer_wrapper, global_config):
    # 简单的向量检索 + LLM生成
    results = await chunks_vdb.query(query, top_k=query_param.top_k)
    chunks = await text_chunks_db.get_by_ids(chunks_ids)
    # 直接基于检索到的文本块生成回答
```

### 3. _llm.py - LLM接口模块 (9.7KB, 294行)

提供多平台LLM和嵌入服务支持。

#### 支持的平台:
- **OpenAI**: GPT-4o, GPT-4o-mini, text-embedding-3-small
- **Azure OpenAI**: 对应的Azure版本
- **Amazon Bedrock**: Claude-3 Sonnet/Haiku, Titan embeddings

#### 核心特性:
```python
# 缓存机制
async def openai_complete_if_cache(model, prompt, system_prompt=None, **kwargs):
    if hashing_kv is not None:
        args_hash = compute_args_hash(model, messages)
        if_cache_return = await hashing_kv.get_by_id(args_hash)
        if if_cache_return is not None:
            return if_cache_return["return"]
    # ... LLM调用和缓存保存

# 重试机制
@retry(stop=stop_after_attempt(5), 
       wait=wait_exponential(multiplier=1, min=4, max=10),
       retry=retry_if_exception_type((RateLimitError, APIConnectionError)))
```

### 4. _utils.py - 工具函数模块 (11KB, 305行)

提供系统所需的各种工具函数。

#### 核心组件:

##### TokenizerWrapper - 分词器包装:
```python
class TokenizerWrapper:
    def __init__(self, tokenizer_type: Literal["tiktoken", "huggingface"] = "tiktoken", 
                 model_name: str = "gpt-4o"):
        # 支持tiktoken和huggingface两种分词器
    
    def encode(self, text: str) -> list[int]
    def decode(self, tokens: list[int]) -> str
    def decode_batch(self, tokens_list: list[list[int]]) -> list[str]
```

##### 重要工具函数:
```python
def truncate_list_by_token_size(list_data, key, max_token_size, tokenizer_wrapper):
    # 按token数量截断列表，确保不超过限制

def convert_response_to_json(response: str) -> dict:
    # 解析LLM返回的JSON，支持容错

def limit_async_func_call(max_size: int):
    # 限制并发调用数量的装饰器
```

### 5. base.py - 基础接口定义 (5.4KB, 186行)

定义了系统的基础接口和数据结构。

#### 核心接口:
```python
@dataclass
class BaseVectorStorage(StorageNameSpace):
    async def query(self, query: str, top_k: int) -> list[dict]
    async def upsert(self, data: dict[str, dict])

@dataclass  
class BaseGraphStorage(StorageNameSpace):
    async def get_node(self, node_id: str) -> Union[dict, None]
    async def get_edge(self, source_node_id: str, target_node_id: str) -> Union[dict, None]
    async def upsert_node(self, node_id: str, node_data: dict[str, str])
    async def upsert_edge(self, source_node_id: str, target_node_id: str, edge_data: dict[str, str])
    async def clustering(self, algorithm: str)
    async def community_schema(self) -> dict[str, SingleCommunitySchema]
```

#### 数据结构:
```python
@dataclass
class QueryParam:
    mode: Literal["local", "global", "naive"] = "global"
    level: int = 2  # 社区层级
    top_k: int = 20  # 检索数量
    # 各种token限制配置
```

### 6. prompt.py - 提示词模板 (32KB, 520行)

包含所有LLM交互的提示词模板。

#### 关键提示词:

##### 实体提取提示词:
```python
PROMPTS["entity_extraction"] = """
-Goal-
从文本中识别所有指定类型的实体及其关系

-Steps-
1. 识别实体: entity_name, entity_type, entity_description
2. 识别关系: source_entity, target_entity, relationship_description, relationship_strength
3. 按指定格式输出
"""
```

##### 社区报告提示词:
```python
PROMPTS["community_report"] = """
基于社区的实体和关系，生成综合报告
包含: TITLE, SUMMARY, IMPACT SEVERITY RATING, DETAILED FINDINGS
返回JSON格式
"""
```

##### Local查询提示词:
```python
PROMPTS["local_rag_response"] = """
基于提供的数据表(实体、关系、社区报告、原文)回答问题
不要编造信息，只基于提供的证据
"""
```

### 7. _splitter.py - 文本分割器 (3.5KB, 94行)

实现了基于分隔符的文本分割。

```python
class SeparatorSplitter:
    def __init__(self, separators: List[List[int]], chunk_size: int = 4000, 
                 chunk_overlap: int = 200):
        # 支持多种分隔符优先级
        # 自动合并小块，处理重叠
    
    def split_tokens(self, tokens: List[int]) -> List[List[int]]:
        # 按分隔符分割，然后合并到合适大小
```

## Entity Extraction模块详解

Entity Extraction模块是基于DSPy框架实现的高级实体关系抽取系统，提供了完整的实体识别、关系抽取、自我完善和评估功能。

### 1. module.py - 核心实体关系抽取器 (13KB, 330行)

这是实体抽取模块的核心，基于DSPy框架实现了智能的实体关系抽取系统。

#### 预定义实体类型:
```python
ENTITY_TYPES = [
    "PERSON", "ORGANIZATION", "LOCATION", "DATE", "TIME", "MONEY", 
    "PERCENTAGE", "PRODUCT", "EVENT", "LANGUAGE", "NATIONALITY",
    "RELIGION", "TITLE", "PROFESSION", "ANIMAL", "PLANT", "DISEASE",
    "MEDICATION", "CHEMICAL", "MATERIAL", "COLOR", "SHAPE", 
    "MEASUREMENT", "WEATHER", "NATURAL_DISASTER", "AWARD", "LAW",
    "CRIME", "TECHNOLOGY", "SOFTWARE", "HARDWARE", "VEHICLE",
    "FOOD", "DRINK", "SPORT", "MUSIC_GENRE", "INSTRUMENT",
    "ARTWORK", "BOOK", "MOVIE", "TV_SHOW", "ACADEMIC_SUBJECT",
    "SCIENTIFIC_THEORY", "POLITICAL_PARTY", "CURRENCY", 
    "STOCK_SYMBOL", "FILE_TYPE", "PROGRAMMING_LANGUAGE",
    "MEDICAL_PROCEDURE", "CELESTIAL_BODY"
]
```

#### 数据模型:

##### Entity实体模型:
```python
class Entity(BaseModel):
    entity_name: str          # 实体名称
    entity_type: str          # 实体类型（来自预定义类型）
    description: str          # 详细描述
    importance_score: float   # 重要性评分(0-1)
    
    def to_dict(self):
        return {
            "entity_name": clean_str(self.entity_name.upper()),
            "entity_type": clean_str(self.entity_type.upper()),
            "description": clean_str(self.description),
            "importance_score": float(self.importance_score),
        }
```

##### Relationship关系模型:
```python
class Relationship(BaseModel):
    src_id: str         # 源实体名称
    tgt_id: str         # 目标实体名称
    description: str    # 关系详细描述
    weight: float       # 关系强度(0-1)
    order: int          # 关系阶数(1-3)
    
    # order说明:
    # 1: 直接关系 - 实体间的直接连接
    # 2: 二阶关系 - 由直接关系产生的间接影响
    # 3: 三阶关系 - 由二阶关系产生的进一步间接影响
```

#### DSPy签名和工作流:

##### 1. CombinedExtraction - 联合抽取:
```python
class CombinedExtraction(dspy.Signature):
    """
    从文本中同时提取实体和关系的主要签名
    
    实体提取准则:
    1. 实体名称必须来自原文本的实际词汇
    2. 避免重复和泛型术语
    3. 描述必须详细全面，包含：
       a) 实体在上下文中的角色或意义
       b) 关键属性或特征
       c) 与其他实体的关系
       d) 历史或文化相关性
       e) 相关的重要行为或事件
    
    关系提取准则:
    1. 关系描述必须详细全面，包含：
       a) 关系性质（家庭、职业、因果等）
       b) 关系对双方实体的影响和意义
       c) 相关的历史或上下文信息
       d) 关系的时间演变
       e) 由此关系产生的重要事件或行为
    2. 包含直接关系(order 1)和高阶关系(order 2-3)
    """
    
    input_text: str
    entity_types: list[str]
    entities: list[Entity]
    relationships: list[Relationship]
```

##### 2. CritiqueCombinedExtraction - 批评评估:
```python
class CritiqueCombinedExtraction(dspy.Signature):
    """
    对当前提取结果进行批评和评估
    
    评估准则:
    1. 评估是否捕获了所有相关实体并正确分类
    2. 检查实体描述是否全面并遵循准则
    3. 评估关系提取的完整性，包括高阶关系
    4. 验证关系描述是否详细并遵循准则
    5. 识别当前提取中的不一致、错误或遗漏
    6. 建议具体的改进或补充
    """
    
    input_text: str
    entity_types: list[str]
    current_entities: list[Entity]
    current_relationships: list[Relationship]
    entity_critique: str
    relationship_critique: str
```

##### 3. RefineCombinedExtraction - 精炼改进:
```python
class RefineCombinedExtraction(dspy.Signature):
    """
    基于批评反馈精炼提取结果
    
    改进准则:
    1. 解决批评中提出的所有问题
    2. 添加批评中识别的缺失实体和关系
    3. 按建议改进实体和关系描述
    4. 确保所有改进仍遵循原始提取准则
    5. 保持实体和关系间的一致性
    6. 专注于提升提取的整体质量和完整性
    """
    
    input_text: str
    entity_types: list[str]
    current_entities: list[Entity]
    current_relationships: list[Relationship]
    entity_critique: str
    relationship_critique: str
    refined_entities: list[Entity]
    refined_relationships: list[Relationship]
```

#### 核心抽取器:

##### TypedEntityRelationshipExtractor:
```python
class TypedEntityRelationshipExtractor(dspy.Module):
    def __init__(
        self,
        lm: dspy.LM = None,              # 语言模型
        max_retries: int = 3,            # 最大重试次数
        entity_types: list[str] = ENTITY_TYPES,  # 实体类型列表
        self_refine: bool = False,       # 是否启用自我完善
        num_refine_turns: int = 1,       # 完善轮数
    ):
        # 初始化各个组件
        self.extractor = dspy.ChainOfThought(signature=CombinedExtraction)
        if self_refine:
            self.critique = dspy.ChainOfThought(signature=CritiqueCombinedExtraction)
            self.refine = dspy.ChainOfThought(signature=RefineCombinedExtraction)
    
    def forward(self, input_text: str) -> dspy.Prediction:
        # 1. 初始提取
        extraction_result = self.extractor(
            input_text=input_text, 
            entity_types=self.entity_types
        )
        
        current_entities = extraction_result.entities
        current_relationships = extraction_result.relationships
        
        # 2. 自我完善循环（可选）
        if self.self_refine:
            for _ in range(self.num_refine_turns):
                # 批评当前结果
                critique_result = self.critique(
                    input_text=input_text,
                    entity_types=self.entity_types,
                    current_entities=current_entities,
                    current_relationships=current_relationships,
                )
                
                # 基于批评进行精炼
                refined_result = self.refine(
                    input_text=input_text,
                    entity_types=self.entity_types,
                    current_entities=current_entities,
                    current_relationships=current_relationships,
                    entity_critique=critique_result.entity_critique,
                    relationship_critique=critique_result.relationship_critique,
                )
                
                current_entities = refined_result.refined_entities
                current_relationships = refined_result.refined_relationships
        
        # 3. 转换为字典格式并返回
        entities = [entity.to_dict() for entity in current_entities]
        relationships = [relationship.to_dict() for relationship in current_relationships]
        
        return dspy.Prediction(entities=entities, relationships=relationships)
```

### 2. extract.py - 提取流程实现 (6.2KB, 171行)

实现了实体提取的具体流程，整合到nano_graphrag系统中。

#### 核心函数:

##### generate_dataset - 数据集生成:
```python
async def generate_dataset(
    chunks: dict[str, TextChunkSchema],
    filepath: str,
    save_dataset: bool = True,
    global_config: dict = {},
) -> list[dspy.Example]:
    """
    从文本块生成实体关系抽取的训练数据集
    
    流程:
    1. 初始化TypedEntityRelationshipExtractor
    2. 并发处理所有文本块
    3. 生成DSPy Example格式的训练数据
    4. 过滤空结果并保存
    """
    
    entity_extractor = TypedEntityRelationshipExtractor(
        num_refine_turns=1, 
        self_refine=True
    )
    
    # 并发处理所有块
    examples = await asyncio.gather(
        *[_process_single_content(c) for c in ordered_chunks]
    )
    
    # 过滤有效样本并保存
    filtered_examples = [
        example for example in examples
        if len(example.entities) > 0 and len(example.relationships) > 0
    ]
    
    if save_dataset:
        with open(filepath, "wb") as f:
            pickle.dump(filtered_examples, f)
    
    return filtered_examples
```

##### extract_entities_dspy - 主要提取函数:
```python
async def extract_entities_dspy(
    chunks: dict[str, TextChunkSchema],
    knwoledge_graph_inst: BaseGraphStorage,
    entity_vdb: BaseVectorStorage,
    global_config: dict,
) -> Union[BaseGraphStorage, None]:
    """
    使用DSPy从文本块中提取实体和关系，并存储到图数据库
    
    流程:
    1. 初始化实体抽取器
    2. 并发处理所有文本块
    3. 收集和合并同名实体/关系
    4. 批量存储到图数据库
    5. 向量化实体并存储到向量数据库
    """
    
    entity_extractor = TypedEntityRelationshipExtractor(
        num_refine_turns=1, 
        self_refine=True
    )
    
    # 并发处理文本块
    async def _process_single_content(chunk_key_dp):
        chunk_key, chunk_dp = chunk_key_dp
        content = chunk_dp["content"]
        
        # 抽取实体和关系
        prediction = await asyncio.to_thread(
            entity_extractor, 
            input_text=content
        )
        entities, relationships = prediction.entities, prediction.relationships
        
        # 整理为节点和边格式
        maybe_nodes = defaultdict(list)
        maybe_edges = defaultdict(list)
        
        for entity in entities:
            entity["source_id"] = chunk_key
            maybe_nodes[entity["entity_name"]].append(entity)
        
        for relationship in relationships:
            relationship["source_id"] = chunk_key
            maybe_edges[(relationship["src_id"], relationship["tgt_id"])].append(
                relationship
            )
        
        return dict(maybe_nodes), dict(maybe_edges)
    
    # 执行并发处理
    results = await asyncio.gather(
        *[_process_single_content(c) for c in ordered_chunks]
    )
    
    # 合并所有结果
    maybe_nodes = defaultdict(list)
    maybe_edges = defaultdict(list)
    for m_nodes, m_edges in results:
        for k, v in m_nodes.items():
            maybe_nodes[k].extend(v)
        for k, v in m_edges.items():
            maybe_edges[k].extend(v)
    
    # 批量存储节点和边
    all_entities_data = await asyncio.gather(
        *[_merge_nodes_then_upsert(k, v, knwoledge_graph_inst, global_config)
          for k, v in maybe_nodes.items()]
    )
    
    await asyncio.gather(
        *[_merge_edges_then_upsert(k[0], k[1], v, knwoledge_graph_inst, global_config)
          for k, v in maybe_edges.items()]
    )
    
    # 向量化实体并存储
    if entity_vdb is not None:
        data_for_vdb = {
            compute_mdhash_id(dp["entity_name"], prefix="ent-"): {
                "content": dp["entity_name"] + dp["description"],
                "entity_name": dp["entity_name"],
            }
            for dp in all_entities_data
        }
        await entity_vdb.upsert(data_for_vdb)
    
    return knwoledge_graph_inst
```

### 3. metric.py - 评估指标 (2.5KB, 62行)

提供实体关系抽取任务的评估指标。

#### 核心评估指标:

##### relationships_similarity_metric - 关系相似度:
```python
class AssessRelationships(dspy.Signature):
    """
    评估黄金标准和预测关系之间的相似度:
    1. 基于src_id和tgt_id对匹配关系，允许实体名称的轻微变化
    2. 对匹配的对比较：
       a) 描述相似度（语义含义）
       b) 权重相似度
       c) 阶数相似度
    3. 将未匹配的关系视为惩罚
    4. 聚合分数，考虑精确率和召回率
    5. 返回0-1之间的最终相似度分数
    """
    
    gold_relationships: list[Relationship]
    predicted_relationships: list[Relationship]
    similarity_score: float

def relationships_similarity_metric(
    gold: dspy.Example, pred: dspy.Prediction, trace=None
) -> float:
    """使用LLM评估关系相似度的智能指标"""
    model = dspy.ChainOfThought(AssessRelationships)
    gold_relationships = [Relationship(**item) for item in gold["relationships"]]
    predicted_relationships = [Relationship(**item) for item in pred["relationships"]]
    
    similarity_score = float(
        model(
            gold_relationships=gold_relationships,
            predicted_relationships=predicted_relationships,
        ).similarity_score
    )
    return similarity_score
```

##### entity_recall_metric - 实体召回率:
```python
def entity_recall_metric(
    gold: dspy.Example, pred: dspy.Prediction, trace=None
) -> float:
    """计算实体抽取的召回率"""
    true_set = set(item["entity_name"] for item in gold["entities"])
    pred_set = set(item["entity_name"] for item in pred["entities"])
    
    true_positives = len(pred_set.intersection(true_set))
    false_negatives = len(true_set - pred_set)
    
    recall = (
        true_positives / (true_positives + false_negatives)
        if (true_positives + false_negatives) > 0
        else 0
    )
    return recall
```

### Entity Extraction模块特点总结

#### 🎯 核心优势:
1. **基于DSPy框架**: 利用程序化提示工程，提高抽取质量和一致性
2. **自我完善机制**: 通过批评-精炼循环持续改进抽取结果
3. **多阶关系处理**: 支持直接关系、二阶关系和三阶关系的识别
4. **详细描述要求**: 强制生成丰富的实体和关系描述，增强知识表示
5. **并发处理**: 异步处理大量文本块，提升处理效率
6. **智能评估**: 基于LLM的语义相似度评估，而非简单字符串匹配

#### 🔧 技术特性:
1. **预定义实体类型**: 62种常见实体类型，覆盖广泛领域
2. **异常处理**: 优雅处理LLM API错误，确保系统稳定性
3. **批量存储**: 高效的图数据库和向量数据库存储
4. **重要性评分**: 为实体和关系分配重要性权重
5. **源追踪**: 记录每个实体和关系的来源文本块

#### 🚀 工作流程:
```
文本输入 → DSPy实体关系抽取 → 自我批评 → 精炼改进 → 合并去重 → 图存储 → 向量化
```

这个模块为nano_graphrag提供了强大的实体关系抽取能力，是构建高质量知识图谱的核心组件。

## 存储层实现

### 1. kv_json.py - JSON键值存储

简单的基于JSON文件的键值存储，用于存储文档、chunks、社区报告等。

```python
@dataclass
class JsonKVStorage(BaseKVStorage):
    # 内存中维护数据，定期写入JSON文件
    # 支持批量操作和字段过滤
```

### 2. vdb_nanovectordb.py - 向量数据库

基于nano_vectordb的向量存储实现。

```python
@dataclass
class NanoVectorDBStorage(BaseVectorStorage):
    # 支持批量嵌入生成
    # 余弦相似度检索
    # 自动保存机制
```

### 3. gdb_neo4j.py - Neo4j图数据库

完整的Neo4j图数据库实现，支持所有图操作。

#### 核心功能:
```python
# 图聚类 - 使用Leiden算法
async def clustering(self, algorithm: str):
    # 1. 投影图为无向图
    # 2. 运行Leiden聚类算法  
    # 3. 将社区ID写回节点
    
# 社区模式提取
async def community_schema(self) -> dict[str, SingleCommunitySchema]:
    # 基于节点的communityIds属性构建社区结构
    # 计算社区包含的节点、边、文本块
    # 确定子社区关系
```

## 系统工作流程

### 1. 文档插入流程
```
文档输入 → 文本分块 → 实体关系提取 → 图存储 → 图聚类 → 社区报告生成 → 向量化存储
```

1. **文本分块**: 按token大小分割，支持重叠
2. **实体提取**: LLM提取实体和关系，支持多轮gleaning
3. **图构建**: 将实体和关系存储到图数据库
4. **图聚类**: 使用Leiden算法进行社区检测
5. **社区报告**: 为每个社区生成结构化报告
6. **向量化**: 对实体描述和文本块进行向量化

### 2. 查询流程

#### Local查询 (推荐):
```
查询 → 实体向量检索 → 图扩展 → 社区匹配 → 上下文构建 → LLM生成回答
```

#### Global查询:
```  
查询 → 社区排序 → Map分析 → Reduce合并 → LLM生成回答
```

#### Naive查询:
```
查询 → 文本向量检索 → LLM生成回答  
```

## 图检索的核心机制详解

### 1. 实体中心检索
- **向量检索**: 使用embedding相似度找到相关实体
- **图扩展**: 基于实体的邻居关系扩展检索范围
- **度数排序**: 使用节点和边的度数作为重要性指标

### 2. 社区聚合
- **层次聚类**: 支持多层社区结构
- **社区报告**: 预生成的结构化社区摘要
- **子社区处理**: 自动处理层次关系，避免重复信息

### 3. 上下文构建
- **多源信息**: 整合实体、关系、社区、原文
- **结构化输出**: CSV格式的表格化数据
- **Token管理**: 智能截断，确保不超过模型限制

### 4. 分布式处理
- **并发控制**: 限制并发LLM调用数量
- **批量操作**: 数据库批量查询优化
- **缓存机制**: LLM响应缓存，减少重复调用

## 使用示例

### 基本使用:
```python
from nano_graphrag import GraphRAG, QueryParam

# 初始化
rag = GraphRAG(working_dir="./cache")

# 插入文档
with open("document.txt") as f:
    rag.insert(f.read())

# 查询
result = rag.query("What are the main themes?", 
                   param=QueryParam(mode="local"))
```

### 使用本地嵌入:
```python
from sentence_transformers import SentenceTransformer

@wrap_embedding_func_with_attrs(embedding_dim=384, max_token_size=512)
async def local_embedding(texts: list[str]) -> np.ndarray:
    return model.encode(texts, normalize_embeddings=True)

rag = GraphRAG(working_dir="./cache", embedding_func=local_embedding)
```

## 总结

Nano GraphRAG是一个精心设计的轻量级GraphRAG实现，具有以下特点:

**优势:**
1. **模块化设计**: 清晰的分层架构，易于扩展
2. **多平台支持**: 支持OpenAI、Azure、Bedrock等多个LLM平台  
3. **灵活的存储**: 支持JSON、NanoVectorDB、Neo4j等多种存储后端
4. **优化的检索**: 三种查询模式适应不同场景
5. **图结构增强**: 利用实体关系和社区结构增强检索效果

**核心创新:**
1. **Local查询机制**: 基于实体邻居和社区的图结构检索
2. **分层社区处理**: 支持多层社区结构和子社区关系
3. **Token智能管理**: 动态分配token预算，确保最优信息密度
4. **并发优化**: 异步处理和并发控制，提升性能

这个项目为GraphRAG技术提供了一个高质量的参考实现，特别适合需要深度定制或研究GraphRAG原理的开发者使用。
